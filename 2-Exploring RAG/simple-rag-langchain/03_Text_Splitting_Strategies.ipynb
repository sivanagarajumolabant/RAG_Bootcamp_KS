{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÇÔ∏è Notebook 03: Text Splitting Strategies\n",
    "\n",
    "**LangChain 1.0.5+ | Mixed Level Class**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "1. Understand **why** text splitting is necessary for RAG\n",
    "2. Master **RecursiveCharacterTextSplitter** (the recommended default)\n",
    "3. Learn other splitters: Character, HTMLHeader, RecursiveJson, Token\n",
    "4. Choose optimal **chunk sizes** and **overlap**\n",
    "5. Compare splitters side-by-side\n",
    "6. Apply the right splitter for different content types\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ Table of Contents\n",
    "\n",
    "1. [Why Split Text?](#why-split)\n",
    "2. [RecursiveCharacterTextSplitter](#recursive-splitter)\n",
    "3. [CharacterTextSplitter](#character-splitter)\n",
    "4. [HTMLHeaderTextSplitter](#html-splitter)\n",
    "5. [RecursiveJsonSplitter](#json-splitter)\n",
    "6. [TokenTextSplitter](#token-splitter)\n",
    "7. [Chunk Size & Overlap Optimization](#optimization)\n",
    "8. [Comparison & Best Practices](#comparison)\n",
    "9. [Summary & Exercises](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment ready\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Environment ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"why-split\"></a>\n",
    "## 1. Why Split Text? ü§î\n",
    "\n",
    "### üî∞ BEGINNER EXPLANATION\n",
    "\n",
    "Imagine you have a 200-page book and someone asks: *\"What did the author say about machine learning on page 87?\"*\n",
    "\n",
    "**Problem:** LLMs have a limited \"attention span\" (context window):\n",
    "- GPT-3.5-Turbo: ~4,000 tokens (~16,000 characters)\n",
    "- GPT-4: ~8,000 tokens (~32,000 characters)\n",
    "- You **can't** fit a whole book in one query!\n",
    "\n",
    "**Solution:** Split the book into smaller **chunks**:\n",
    "1. Each chunk is small enough for the LLM\n",
    "2. Search finds the **relevant chunks** (like page 87)\n",
    "3. Only send those chunks to the LLM\n",
    "\n",
    "### The Challenge\n",
    "\n",
    "If you split text randomly:\n",
    "```\n",
    "‚ùå BAD SPLIT:\n",
    "Chunk 1: \"The transformer architecture revolutionized NLP. It uses self-att\"\n",
    "Chunk 2: \"ention mechanisms to process sequences in parallel. This allows...\"\n",
    "```\n",
    "\n",
    "The word \"attention\" is cut in half! üò±\n",
    "\n",
    "**Good splitters** respect boundaries (paragraphs, sentences, words):\n",
    "```\n",
    "‚úÖ GOOD SPLIT:\n",
    "Chunk 1: \"The transformer architecture revolutionized NLP. It uses self-attention mechanisms.\"\n",
    "Chunk 2: \"Self-attention allows the model to process sequences in parallel. This improves speed...\"\n",
    "```\n",
    "\n",
    "### üéì INTERMEDIATE: Trade-offs\n",
    "\n",
    "| Aspect | Small Chunks (500 chars) | Large Chunks (2000 chars) |\n",
    "|--------|-------------------------|---------------------------|\n",
    "| **Precision** | High (very specific) | Lower (more general) |\n",
    "| **Context** | Less context | More context |\n",
    "| **Retrieval Quality** | More precise matches | May include noise |\n",
    "| **# of Chunks** | More chunks = more storage | Fewer chunks |\n",
    "| **Best for** | Q&A, facts, technical docs | Long-form, narrative content |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"recursive-splitter\"></a>\n",
    "## 2. RecursiveCharacterTextSplitter ‚≠ê\n",
    "\n",
    "### üî∞ BEGINNER: The Default Choice\n",
    "\n",
    "**RecursiveCharacterTextSplitter** is your go-to splitter for 90% of cases.\n",
    "\n",
    "**How it works:**\n",
    "1. Tries to split on **double newlines** (\\n\\n) ‚Üí paragraphs\n",
    "2. If chunks still too big, splits on **single newlines** (\\n) ‚Üí lines\n",
    "3. If still too big, splits on **periods** (.) ‚Üí sentences\n",
    "4. If still too big, splits on **spaces** ( ) ‚Üí words\n",
    "5. Last resort: splits on **characters**\n",
    "\n",
    "This preserves meaning as much as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Original document: 8567 characters\n",
      "\n",
      "‚úÇÔ∏è Split into 12 chunks\n",
      "\n",
      "======================================================================\n",
      "Chunk 1 (991 chars):\n",
      "======================================================================\n",
      "LANGCHAIN STUDY NOTES - RAG IMPLEMENTATION\n",
      "==========================================\n",
      "\n",
      "Date: January 15, 2025\n",
      "Topic: Retrieval-Augmented Generation with LangChain 1.0+\n",
      "\n",
      "\n",
      "CORE CONCEPTS\n",
      "-------------\n",
      "\n",
      "1. Document Object Structure\n",
      "   - page_content: The actual text content\n",
      "   - metadata: Dictionary wit...\n",
      "\n",
      "======================================================================\n",
      "Chunk 2 (801 chars):\n",
      "======================================================================\n",
      "TEXT SPLITTING STRATEGIES\n",
      "--------------------------\n",
      "\n",
      "RecursiveCharacterTextSplitter (RECOMMENDED)\n",
      "- Tries to split on semantic boundaries\n",
      "- Order: double newline ‚Üí newline ‚Üí period ‚Üí space ‚Üí character\n",
      "- Best for general text and documentation\n",
      "- Configuration: chunk_size=1000, chunk_overlap=200\n",
      "\n",
      "Cha...\n",
      "\n",
      "======================================================================\n",
      "Chunk 3 (864 chars):\n",
      "======================================================================\n",
      "TokenTextSplitter\n",
      "- Splits based on token count, not characters\n",
      "- More accurate for LLM context window limits\n",
      "- Uses tiktoken for OpenAI models\n",
      "\n",
      "\n",
      "CHUNK SIZE GUIDELINES\n",
      "----------------------\n",
      "\n",
      "Content Type          | Chunk Size | Overlap | Notes\n",
      "----------------------|------------|---------|---------...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load our sample text\n",
    "txt_path = \"sample_data/notes.txt\"\n",
    "\n",
    "if Path(txt_path).exists():\n",
    "    # Load the document\n",
    "    loader = TextLoader(txt_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"üìÑ Original document: {len(documents[0].page_content)} characters\\n\")\n",
    "    \n",
    "    # Create splitter\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # Maximum chunk size in characters\n",
    "        chunk_overlap=200,      # Overlap between chunks\n",
    "        length_function=len,    # How to measure length\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Try these in order\n",
    "    )\n",
    "    \n",
    "    # Split the document\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Split into {len(chunks)} chunks\\n\")\n",
    "    \n",
    "    # Examine first 3 chunks\n",
    "    for i, chunk in enumerate(chunks[:3], 1):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Chunk {i} ({len(chunk.page_content)} chars):\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(chunk.page_content[:300] + \"...\" if len(chunk.page_content) > 300 else chunk.page_content)\n",
    "        print()\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {txt_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ Understanding Chunk Overlap\n",
    "\n",
    "**Why overlap?** To preserve context across boundaries.\n",
    "\n",
    "**Example without overlap:**\n",
    "```\n",
    "Chunk 1: \"...introducing the transformer architecture.\"\n",
    "Chunk 2: \"The model uses multi-head attention...\"\n",
    "```\n",
    "‚Üí Missing connection between \"transformer\" and \"multi-head attention\"\n",
    "\n",
    "**Example with overlap:**\n",
    "```\n",
    "Chunk 1: \"...introducing the transformer architecture. The model uses...\"\n",
    "Chunk 2: \"...transformer architecture. The model uses multi-head attention...\"\n",
    "```\n",
    "‚Üí Both chunks have the connection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Examining overlap between chunks:\n",
      "\n",
      "Chunk 1 ending:\n",
      "  ...ontent: The actual text content\n",
      "   - metadata: Dictionary with additional information (source, page, date, etc.)\n",
      "   - id: Unique identifier (optional)\n",
      "\n",
      "Chunk 2 beginning:\n",
      "  2. LCEL (LangChain Expression Language)\n",
      "   - Uses pipe operator | to chain components\n",
      "   - More readable than nested function calls\n",
      "   - Better error ...\n",
      "\n",
      "üí° Notice the overlap? This preserves context!\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate overlap\n",
    "if Path(txt_path).exists():\n",
    "    docs = TextLoader(txt_path).load()\n",
    "    \n",
    "    # Splitter with overlap\n",
    "    splitter_with_overlap = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100  # 100 chars overlap\n",
    "    )\n",
    "    \n",
    "    chunks = splitter_with_overlap.split_documents(docs)\n",
    "    \n",
    "    print(\"üîç Examining overlap between chunks:\\n\")\n",
    "    \n",
    "    # Show overlap between chunk 1 and 2\n",
    "    if len(chunks) >= 2:\n",
    "        chunk1_end = chunks[0].page_content[-150:]\n",
    "        chunk2_start = chunks[1].page_content[:150]\n",
    "        \n",
    "        print(\"Chunk 1 ending:\")\n",
    "        print(f\"  ...{chunk1_end}\")\n",
    "        print(\"\\nChunk 2 beginning:\")\n",
    "        print(f\"  {chunk2_start}...\")\n",
    "        print(\"\\nüí° Notice the overlap? This preserves context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Custom Separators for Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Split code into 3 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "def calculate_total(items):\n",
      "    \"\"\"Calculate total price of items.\"\"\"\n",
      "    total = 0\n",
      "    for item in items:\n",
      "        total += item['price']\n",
      "    return total\n",
      "--------------------------------------------------\n",
      "Chunk 2:\n",
      "def apply_discount(total, discount_percent):\n",
      "    \"\"\"Apply discount to total.\"\"\"\n",
      "    discount = total * (discount_percent / 100)\n",
      "    return total - discount\n",
      "--------------------------------------------------\n",
      "Chunk 3:\n",
      "class ShoppingCart:\n",
      "    def __init__(self):\n",
      "        self.items = []\n",
      "\n",
      "    def add_item(self, item):\n",
      "        self.items.append(item)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example: Splitting Python code\n",
    "from langchain_text_splitters import Language, RecursiveCharacterTextSplitter\n",
    "\n",
    "# Python code example\n",
    "python_code = '''\n",
    "def calculate_total(items):\n",
    "    \"\"\"Calculate total price of items.\"\"\"\n",
    "    total = 0\n",
    "    for item in items:\n",
    "        total += item['price']\n",
    "    return total\n",
    "\n",
    "def apply_discount(total, discount_percent):\n",
    "    \"\"\"Apply discount to total.\"\"\"\n",
    "    discount = total * (discount_percent / 100)\n",
    "    return total - discount\n",
    "\n",
    "class ShoppingCart:\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "    \n",
    "    def add_item(self, item):\n",
    "        self.items.append(item)\n",
    "'''\n",
    "\n",
    "# Python-aware splitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "code_chunks = python_splitter.split_text(python_code)\n",
    "\n",
    "print(f\"‚úÇÔ∏è Split code into {len(code_chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(code_chunks, 1):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"character-splitter\"></a>\n",
    "## 3. CharacterTextSplitter\n",
    "\n",
    "### üî∞ BEGINNER: Simple Splitting\n",
    "\n",
    "**CharacterTextSplitter** splits on a single separator (like \"\\n\\n\").\n",
    "- Simpler than Recursive\n",
    "- Less intelligent\n",
    "- Use for testing or very simple text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 3 chunks:\n",
      "\n",
      "Chunk 1: First paragraph about machine learning.\n",
      "It has multiple sentences. This is important context.\n",
      "\n",
      "Chunk 2: Second paragraph about deep learning.\n",
      "Neural networks are powerful. They learn from data.\n",
      "\n",
      "Chunk 3: Third paragraph about transformers.\n",
      "Attention mechanisms are key. They revolutionized NLP.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Sample text with clear paragraph breaks\n",
    "sample_text = \"\"\"First paragraph about machine learning.\n",
    "It has multiple sentences. This is important context.\n",
    "\n",
    "Second paragraph about deep learning.\n",
    "Neural networks are powerful. They learn from data.\n",
    "\n",
    "Third paragraph about transformers.\n",
    "Attention mechanisms are key. They revolutionized NLP.\n",
    "\"\"\"\n",
    "\n",
    "# Split on paragraph breaks\n",
    "simple_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",  # Split on double newline (paragraphs)\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "chunks = simple_splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"html-splitter\"></a>\n",
    "## 4. HTMLHeaderTextSplitter üåê\n",
    "\n",
    "### üî∞ BEGINNER: Structure-Aware Splitting\n",
    "\n",
    "**HTMLHeaderTextSplitter** splits HTML based on headers (h1, h2, h3).\n",
    "- Preserves document structure\n",
    "- Adds header information to metadata\n",
    "- Perfect for documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Split HTML into 48 sections\n",
      "\n",
      "======================================================================\n",
      "Section 1:\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG'}\n",
      "Content (first 200 chars): Building Intelligent Applications with RAG...\n",
      "\n",
      "======================================================================\n",
      "Section 2:\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG'}\n",
      "Content (first 200 chars): | |  \n",
      "By Dr. Amanda Foster  \n",
      "January 15, 2025  \n",
      "12 min read...\n",
      "\n",
      "======================================================================\n",
      "Section 3:\n",
      "Metadata: {'Title': 'Building Intelligent Applications with RAG', 'Section': 'Introduction'}\n",
      "Content (first 200 chars): Introduction...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "# Load the HTML blog post\n",
    "html_path = \"sample_data/blog_post.html\"\n",
    "\n",
    "if Path(html_path).exists():\n",
    "    # Read HTML content\n",
    "    with open(html_path, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Define headers to split on\n",
    "    headers_to_split_on = [\n",
    "        (\"h1\", \"Title\"),\n",
    "        (\"h2\", \"Section\"),\n",
    "        (\"h3\", \"Subsection\"),\n",
    "    ]\n",
    "    \n",
    "    # Create splitter\n",
    "    html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    \n",
    "    # Split the HTML\n",
    "    html_chunks = html_splitter.split_text(html_content)\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Split HTML into {len(html_chunks)} sections\\n\")\n",
    "    \n",
    "    # Show first 3 sections with metadata\n",
    "    for i, chunk in enumerate(html_chunks[:3], 1):\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Section {i}:\")\n",
    "        print(f\"Metadata: {chunk.metadata}\")\n",
    "        print(f\"Content (first 200 chars): {chunk.page_content[:200]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(f\"‚ùå HTML file not found: {html_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"json-splitter\"></a>\n",
    "## 5. RecursiveJsonSplitter üì¶\n",
    "\n",
    "### üî∞ BEGINNER: Splitting JSON Data\n",
    "\n",
    "**RecursiveJsonSplitter** splits JSON while preserving structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Split JSON into 7 chunks\n",
      "\n",
      "First chunk:\n",
      "\"{\\\"api_version\\\": \\\"v2.0\\\", \\\"timestamp\\\": \\\"2025-01-15T10:30:00Z\\\", \\\"total_results\\\": 5, \\\"articles\\\": {\\\"0\\\": {\\\"id\\\": \\\"article_001\\\", \\\"title\\\": \\\"Introduction to Retrieval-Augmented Generation (RAG)\\\", \\\"author\\\": \\\"Dr. Sarah Chen\\\", \\\"published_date\\\": \\\"2025-01-10\\\", \\\"category\\\": \\\"Machine Learning\\\", \\\"tags\\\": {\\\"0\\\": \\\"RAG\\\", \\\"1\\\": \\\"LLM\\\", \\\"2\\\": \\\"NLP\\\", \\\"3\\\": \\\"AI\\\"}, \\\"summary\\\": \\\"Retrieval-Augmented Generation (RAG) is a powerful technique that combines information retrieval ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "\n",
    "# Load JSON data\n",
    "json_path = \"sample_data/api_response.json\"\n",
    "\n",
    "if Path(json_path).exists():\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "    \n",
    "    # Create splitter\n",
    "    json_splitter = RecursiveJsonSplitter(\n",
    "        max_chunk_size=1000,\n",
    "        min_chunk_size=100\n",
    "    )\n",
    "    \n",
    "    # Split\n",
    "    json_chunks = json_splitter.split_text(\n",
    "        json_data=json_data,\n",
    "        convert_lists=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Split JSON into {len(json_chunks)} chunks\\n\")\n",
    "    \n",
    "    # Show first chunk\n",
    "    print(\"First chunk:\")\n",
    "    print(json.dumps(json_chunks[0], indent=2)[:500] + \"...\")\n",
    "else:\n",
    "    print(f\"‚ùå JSON file not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"token-splitter\"></a>\n",
    "## 6. TokenTextSplitter üéØ\n",
    "\n",
    "### üéì INTERMEDIATE: Precise Token-Based Splitting\n",
    "\n",
    "**TokenTextSplitter** splits based on **tokens** (not characters).\n",
    "- More accurate for LLM context windows\n",
    "- Uses tiktoken (OpenAI's tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 1 token-based chunks:\n",
      "\n",
      "Chunk 1: The transformer architecture, introduced in the paper 'Attention Is All You Need', \n",
      "revolutionized natural language processing. It uses self-attention mechanisms to process \n",
      "sequences in parallel, making it much faster than recurrent neural networks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"The transformer architecture, introduced in the paper 'Attention Is All You Need', \n",
    "revolutionized natural language processing. It uses self-attention mechanisms to process \n",
    "sequences in parallel, making it much faster than recurrent neural networks.\"\"\"\n",
    "\n",
    "# Token-based splitter\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=50,  # 50 tokens (not characters!)\n",
    "    chunk_overlap=10,\n",
    "    encoding_name=\"cl100k_base\"  # GPT-3.5/GPT-4 tokenizer\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(text)\n",
    "\n",
    "print(f\"Split into {len(token_chunks)} token-based chunks:\\n\")\n",
    "for i, chunk in enumerate(token_chunks, 1):\n",
    "    print(f\"Chunk {i}: {chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"optimization\"></a>\n",
    "## 7. Chunk Size & Overlap Optimization üìä\n",
    "\n",
    "### üî∞ BEGINNER: Rules of Thumb\n",
    "\n",
    "#### Recommended Configurations\n",
    "\n",
    "| Content Type | Chunk Size | Overlap | Why |\n",
    "|-------------|-----------|---------|-----|\n",
    "| **General Text** | 1000 chars | 200 chars | Balanced precision & context |\n",
    "| **Technical Docs** | 500-800 | 100-150 | Precision for code/commands |\n",
    "| **Long Articles** | 1500-2000 | 300 | More context for narrative |\n",
    "| **Code** | 200-400 | 50-100 | Function/class level |\n",
    "| **FAQs** | 200-300 | 30-50 | Question-answer pairs |\n",
    "\n",
    "#### Overlap Guidelines\n",
    "- **10-15%**: Minimal overlap, saves storage\n",
    "- **20%**: Sweet spot (recommended)\n",
    "- **30%+**: Maximum context preservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chunk Size Comparison:\n",
      "\n",
      "Size     Chunks     Avg Length   Overlap %\n",
      "--------------------------------------------------\n",
      "500      25         356          20%\n",
      "1000     12         819          20%\n",
      "1500     8          1229         20%\n",
      "2000     6          1650         20%\n"
     ]
    }
   ],
   "source": [
    "# Compare different chunk sizes\n",
    "if Path(txt_path).exists():\n",
    "    docs = TextLoader(txt_path).load()\n",
    "    \n",
    "    chunk_sizes = [500, 1000, 1500, 2000]\n",
    "    \n",
    "    print(\"üìä Chunk Size Comparison:\\n\")\n",
    "    print(f\"{'Size':<8} {'Chunks':<10} {'Avg Length':<12} {'Overlap %'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in chunk_sizes:\n",
    "        overlap = int(size * 0.2)  # 20% overlap\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=size,\n",
    "            chunk_overlap=overlap\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        avg_length = sum(len(c.page_content) for c in chunks) / len(chunks)\n",
    "        overlap_pct = (overlap / size) * 100\n",
    "        \n",
    "        print(f\"{size:<8} {len(chunks):<10} {avg_length:<12.0f} {overlap_pct:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 8. Comparison & Best Practices üåü\n",
    "\n",
    "### Splitter Comparison\n",
    "\n",
    "| Splitter | Best For | Pros | Cons |\n",
    "|----------|----------|------|------|\n",
    "| **RecursiveCharacter** | General text, docs | Smart boundaries, flexible | Slower |\n",
    "| **Character** | Simple text | Fast, simple | Not intelligent |\n",
    "| **HTMLHeader** | Web content, docs | Preserves structure | HTML only |\n",
    "| **RecursiveJson** | JSON data | Preserves JSON structure | JSON only |\n",
    "| **Token** | Precise LLM usage | Accurate token count | Requires tokenizer |\n",
    "\n",
    "### üéì Best Practices\n",
    "\n",
    "1. **Start with RecursiveCharacterTextSplitter**\n",
    "2. **Test different chunk sizes** with your data\n",
    "3. **Use 20% overlap** as default\n",
    "4. **Match splitter to content type** (HTML ‚Üí HTMLHeaderTextSplitter)\n",
    "5. **Monitor retrieval quality** and adjust\n",
    "6. **Consider token-based splitting** for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 9. Summary & Exercises üìù\n",
    "\n",
    "### üéâ What You Learned\n",
    "\n",
    "‚úÖ Text splitting is necessary because **LLMs have limited context windows**\n",
    "\n",
    "‚úÖ **RecursiveCharacterTextSplitter** is the recommended default\n",
    "\n",
    "‚úÖ **Chunk size** determines precision vs context trade-off\n",
    "\n",
    "‚úÖ **Overlap** (20%) preserves context across boundaries\n",
    "\n",
    "‚úÖ Different content types need different splitters\n",
    "\n",
    "‚úÖ **Best practice:** chunk_size=1000, chunk_overlap=200 for general text\n",
    "\n",
    "### üí° Practice Exercises\n",
    "\n",
    "#### üî∞ Beginner\n",
    "1. Load a PDF and split it with chunk_size=500, overlap=100\n",
    "2. Count total chunks created\n",
    "3. Print first and last chunks\n",
    "\n",
    "#### üéì Intermediate\n",
    "1. Compare chunk sizes (500, 1000, 2000) on the same document\n",
    "2. Create a chart showing # of chunks vs chunk size\n",
    "3. Test different overlap percentages (10%, 20%, 30%)\n",
    "\n",
    "### üìö Next: Notebook 04 - Embeddings!\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
