{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Stores Tutorial: Qdrant & Weaviate\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "In this tutorial, you'll learn:\n",
    "- What vector stores are and why they're essential for RAG (Retrieval-Augmented Generation)\n",
    "- How to work with **Qdrant** (local, in-memory, and persistent storage)\n",
    "- How to work with **Weaviate** (Docker-based setup)\n",
    "- How to use **Ollama embeddings** for real semantic search\n",
    "- Metadata filtering with different vector store syntaxes\n",
    "- When to use each vector store based on your use case\n",
    "\n",
    "## ğŸ“š What are Vector Stores?\n",
    "\n",
    "Vector stores (also called vector databases) are specialized databases that:\n",
    "1. Store **embeddings** - numerical representations of text\n",
    "2. Enable **semantic search** - finding similar content based on meaning, not just keywords\n",
    "3. Power **RAG systems** - providing relevant context to LLMs\n",
    "\n",
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "Before starting, you should have:\n",
    "- Python 3.8+ installed\n",
    "- Basic understanding of LangChain\n",
    "- Ollama installed and running (for real embeddings)\n",
    "- Docker (optional, for Weaviate)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Installation & Setup\n",
    "\n",
    "### Step 1: Install Python Packages\n",
    "\n",
    "Run these commands in your terminal:\n",
    "\n",
    "```bash\n",
    "# For Qdrant\n",
    "pip install langchain-qdrant qdrant-client\n",
    "\n",
    "# For Weaviate\n",
    "pip install langchain-weaviate weaviate-client\n",
    "\n",
    "# For Ollama embeddings\n",
    "pip install langchain-ollama\n",
    "\n",
    "# Core LangChain\n",
    "pip install langchain-core\n",
    "```\n",
    "\n",
    "### Step 2: Set Up Ollama (for Real Embeddings)\n",
    "\n",
    "```bash\n",
    "# Install Ollama from https://ollama.ai\n",
    "\n",
    "# Start Ollama service\n",
    "ollama serve\n",
    "\n",
    "# Pull the embedding model (in another terminal)\n",
    "ollama pull nomic-embed-text\n",
    "```\n",
    "\n",
    "### Step 3: Set Up Weaviate (Optional - Only for Weaviate Section)\n",
    "\n",
    "```bash\n",
    "# Run Weaviate in Docker\n",
    "docker run -d -p 8080:8080 -p 50051:50051 \\\n",
    "  --name weaviate \\\n",
    "  cr.weaviate.io/semitechnologies/weaviate:latest\n",
    "```\n",
    "\n",
    "### ğŸ“Œ Note\n",
    "If you don't have Ollama set up, the code will show you error messages with instructions. You can still run the Qdrant sections to learn the concepts!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Imports and Setup\n",
    "\n",
    "### Important: Correct Imports for LangChain 1.0+\n",
    "\n",
    "LangChain 1.0+ has reorganized its modules. Here's what you need to know:\n",
    "\n",
    "âœ… **CORRECT:**\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "```\n",
    "\n",
    "âŒ **DEPRECATED (will cause errors):**\n",
    "```python\n",
    "from langchain.schema import Document  # Old way\n",
    "from langchain_core.memory import ...  # Memory not in langchain_core\n",
    "```\n",
    "\n",
    "ğŸ’¡ **For memory/chat history, use:**\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All imports loaded correctly!\n",
      "âœ“ Using langchain_core.documents.Document (correct LangChain 1.0+ import)\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# LangChain core - Document class\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Qdrant imports\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, Filter, FieldCondition, MatchValue\n",
    "\n",
    "# Ollama embeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "print(\"âœ“ All imports loaded correctly!\")\n",
    "print(\"âœ“ Using langchain_core.documents.Document (correct LangChain 1.0+ import)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  Understanding Embeddings\n",
    "\n",
    "### What are Embeddings?\n",
    "\n",
    "**Embeddings** are numerical representations of text that capture semantic meaning. Think of them as coordinates in a high-dimensional space where similar meanings are close together.\n",
    "\n",
    "**Example:**\n",
    "- \"cat\" and \"kitten\" â†’ Similar embeddings (close in vector space)\n",
    "- \"cat\" and \"astronomy\" â†’ Different embeddings (far apart)\n",
    "\n",
    "### The nomic-embed-text Model\n",
    "\n",
    "We're using **nomic-embed-text** from Ollama:\n",
    "- **Dimension:** 768 (each text becomes a list of 768 numbers)\n",
    "- **Purpose:** Optimized for text retrieval and semantic search\n",
    "- **Speed:** Fast enough for real-time applications\n",
    "\n",
    "### How Embeddings Power Search\n",
    "\n",
    "1. **Storage:** Convert documents â†’ embeddings â†’ store in vector database\n",
    "2. **Query:** Convert your question â†’ embedding\n",
    "3. **Search:** Find stored embeddings closest to your query embedding\n",
    "4. **Retrieve:** Return the original documents\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Ollama embeddings (nomic-embed-text)...\n",
      "Make sure Ollama is running: 'ollama serve'\n",
      "\n",
      "âœ“ Ollama embeddings initialized\n",
      "  Model: nomic-embed-text\n",
      "  Dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama embeddings\n",
    "# This connects to your local Ollama service and uses the nomic-embed-text model\n",
    "\n",
    "print(\"Initializing Ollama embeddings (nomic-embed-text)...\")\n",
    "print(\"Make sure Ollama is running: 'ollama serve'\\n\")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "print(\"âœ“ Ollama embeddings initialized\")\n",
    "print(\"  Model: nomic-embed-text\")\n",
    "print(\"  Dimension: 768\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“„ Creating Sample Documents\n",
    "\n",
    "Let's create some sample documents to work with. Each document has:\n",
    "- **page_content:** The actual text content\n",
    "- **metadata:** Additional information (topic, difficulty level, etc.)\n",
    "\n",
    "Metadata is useful for filtering - you can search for documents matching specific criteria!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 3 sample documents:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Metadata: {'topic': 'rag', 'difficulty': 'intermediate'}\n",
      "  2. LangChain simplifies LLM applications\n",
      "     Metadata: {'topic': 'langchain', 'difficulty': 'beginner'}\n",
      "  3. Vector databases enable semantic search\n",
      "     Metadata: {'topic': 'vectordb', 'difficulty': 'intermediate'}\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents with metadata\n",
    "# The Document class comes from langchain_core.documents\n",
    "\n",
    "sample_docs = [\n",
    "    Document(\n",
    "        page_content=\"RAG combines retrieval and generation\",\n",
    "        metadata={\"topic\": \"rag\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain simplifies LLM applications\",\n",
    "        metadata={\"topic\": \"langchain\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector databases enable semantic search\",\n",
    "        metadata={\"topic\": \"vectordb\", \"difficulty\": \"intermediate\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"âœ“ Created 3 sample documents:\")\n",
    "for i, doc in enumerate(sample_docs, 1):\n",
    "    print(f\"  {i}. {doc.page_content}\")\n",
    "    print(f\"     Metadata: {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ—„ï¸ Part 1: Qdrant Vector Store\n",
    "\n",
    "## What is Qdrant?\n",
    "\n",
    "**Qdrant** is a vector database that excels at:\n",
    "- âœ… Local development (no Docker required)\n",
    "- âœ… Fast similarity search\n",
    "- âœ… Flexible storage options (in-memory or persistent)\n",
    "- âœ… Rich metadata filtering\n",
    "\n",
    "## Three Ways to Use Qdrant\n",
    "\n",
    "We'll explore three approaches:\n",
    "\n",
    "1. **In-Memory** - Fast but temporary (lost when script ends)\n",
    "2. **Persistent** - Saved to disk (survives restarts)\n",
    "3. **from_documents** - Easiest method (recommended)\n",
    "\n",
    "## ğŸ” Filter Syntax Reference\n",
    "\n",
    "Different vector stores use different filter syntax:\n",
    "\n",
    "**ChromaDB (simple dict):**\n",
    "```python\n",
    "filter={\"topic\": \"rag\"}\n",
    "```\n",
    "\n",
    "**Qdrant (Filter object):**\n",
    "```python\n",
    "from qdrant_client.models import Filter, FieldCondition, MatchValue\n",
    "filter=Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"metadata.topic\",\n",
    "            match=MatchValue(value=\"rag\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Qdrant In-Memory (No Persistence)\n",
    "\n",
    "### When to Use\n",
    "- Quick testing and experiments\n",
    "- Temporary data that doesn't need to be saved\n",
    "- Learning and prototyping\n",
    "\n",
    "### How It Works\n",
    "1. Create an in-memory client with `location=\":memory:\"`\n",
    "2. Create a collection with vector configuration (size=768, distance=COSINE)\n",
    "3. Add documents and search\n",
    "4. **Data is lost when the program ends**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QDRANT IN-MEMORY EXAMPLE\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/13vbmk7n7fqgmdnqjjwn1bx80000gn/T/ipykernel_35525/3668306292.py:14: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client_memory.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Added documents to Qdrant (in-memory)\n",
      "  Collection: my_collection_memory\n",
      "  Documents: 3\n",
      "  Storage: RAM (temporary)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"QDRANT IN-MEMORY EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Step 1: Create in-memory Qdrant client\n",
    "# The `:memory:` location means data is stored in RAM (not saved to disk)\n",
    "qdrant_client_memory = QdrantClient(location=\":memory:\")\n",
    "\n",
    "# Step 2: Create a collection\n",
    "# - collection_name: identifier for this collection\n",
    "# - size: must match embedding dimension (768 for nomic-embed-text)\n",
    "# - distance: COSINE measures similarity (other options: DOT, EUCLID)\n",
    "qdrant_client_memory.recreate_collection(\n",
    "    collection_name=\"my_collection_memory\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Step 3: Create Qdrant vector store wrapper\n",
    "# This LangChain wrapper makes it easy to work with Qdrant\n",
    "qdrant_store_memory = QdrantVectorStore(\n",
    "    client=qdrant_client_memory,\n",
    "    collection_name=\"my_collection_memory\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Step 4: Add documents to the store\n",
    "# This will automatically:\n",
    "# 1. Convert documents to embeddings using Ollama\n",
    "# 2. Store embeddings + metadata in Qdrant\n",
    "qdrant_store_memory.add_documents(sample_docs)\n",
    "print(\"âœ“ Added documents to Qdrant (in-memory)\")\n",
    "print(\"  Collection: my_collection_memory\")\n",
    "print(\"  Documents: 3\")\n",
    "print(\"  Storage: RAM (temporary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Similarity Search\n",
    "\n",
    "Let's search for documents similar to our query. The vector store will:\n",
    "1. Convert your query to an embedding\n",
    "2. Find the k most similar document embeddings\n",
    "3. Return the original documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "BASIC SIMILARITY SEARCH\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Tell me about RAG'\n",
      "\n",
      "Search results:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Metadata: {'topic': 'rag', 'difficulty': 'intermediate', '_id': 'cc8a99cd662549ecbef3637b14ffab34', '_collection_name': 'my_collection_memory'}\n",
      "  2. Vector databases enable semantic search\n",
      "     Metadata: {'topic': 'vectordb', 'difficulty': 'intermediate', '_id': '739f986d982f460497916a7de762e58e', '_collection_name': 'my_collection_memory'}\n",
      "\n",
      "ğŸ’¡ Notice: The document about 'RAG combines retrieval...' is returned first\n",
      "   because it's semantically most similar to our query!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"BASIC SIMILARITY SEARCH\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Search for documents similar to this query\n",
    "# k=2 means return the top 2 most similar documents\n",
    "results = qdrant_store_memory.similarity_search(\n",
    "    \"Tell me about RAG\",\n",
    "    k=2\n",
    ")\n",
    "\n",
    "print(\"\\nQuery: 'Tell me about RAG'\")\n",
    "print(\"\\nSearch results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"  {i}. {doc.page_content}\")\n",
    "    print(f\"     Metadata: {doc.metadata}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Notice: The document about 'RAG combines retrieval...' is returned first\")\n",
    "print(\"   because it's semantically most similar to our query!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search with Metadata Filtering\n",
    "\n",
    "Sometimes you want to search within a subset of documents. Qdrant uses a **Filter object** for this.\n",
    "\n",
    "#### Filter Structure\n",
    "```python\n",
    "Filter(\n",
    "    must=[...],   # All conditions must match (AND logic)\n",
    "    should=[...], # At least one condition must match (OR logic)\n",
    "    must_not=[...] # No conditions should match (NOT logic)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SEARCH WITH METADATA FILTER\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Tell me about RAG'\n",
      "Filter: topic='rag'\n",
      "\n",
      "Filtered search results:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Metadata: {'topic': 'rag', 'difficulty': 'intermediate', '_id': 'cc8a99cd662549ecbef3637b14ffab34', '_collection_name': 'my_collection_memory'}\n",
      "\n",
      "ğŸ’¡ Only documents with topic='rag' are returned!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"SEARCH WITH METADATA FILTER\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create a filter to only search documents with topic='rag'\n",
    "# Note: We use 'metadata.topic' because metadata is nested\n",
    "qdrant_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(\n",
    "            key=\"metadata.topic\",\n",
    "            match=MatchValue(value=\"rag\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Same search, but only among filtered documents\n",
    "results_filtered = qdrant_store_memory.similarity_search(\n",
    "    \"Tell me about RAG\",\n",
    "    k=2,\n",
    "    filter=qdrant_filter\n",
    ")\n",
    "\n",
    "print(\"\\nQuery: 'Tell me about RAG'\")\n",
    "print(\"Filter: topic='rag'\")\n",
    "print(\"\\nFiltered search results:\")\n",
    "for i, doc in enumerate(results_filtered, 1):\n",
    "    print(f\"  {i}. {doc.page_content}\")\n",
    "    print(f\"     Metadata: {doc.metadata}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Only documents with topic='rag' are returned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Filter Conditions (AND Logic)\n",
    "\n",
    "You can combine multiple conditions. All conditions in the `must` list must match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "MULTIPLE FILTERS EXAMPLE (AND LOGIC)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Filter structure:\n",
      "  must=[\n",
      "    FieldCondition(key='metadata.topic', match='rag'),\n",
      "    FieldCondition(key='metadata.difficulty', match='intermediate')\n",
      "  ]\n",
      "\n",
      "ğŸ’¡ Both conditions must be true (AND logic)\n",
      "ğŸ’¡ For OR logic, use should=[...] instead of must=[...]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"MULTIPLE FILTERS EXAMPLE (AND LOGIC)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Filter for documents where:\n",
    "# - topic='rag' AND\n",
    "# - difficulty='intermediate'\n",
    "multi_filter = Filter(\n",
    "    must=[\n",
    "        FieldCondition(key=\"metadata.topic\", match=MatchValue(value=\"rag\")),\n",
    "        FieldCondition(key=\"metadata.difficulty\", match=MatchValue(value=\"intermediate\"))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# You can use this filter in similarity_search:\n",
    "# results_multi = qdrant_store_memory.similarity_search(\"RAG\", k=2, filter=multi_filter)\n",
    "\n",
    "print(\"\\nFilter structure:\")\n",
    "print(\"  must=[\")\n",
    "print(\"    FieldCondition(key='metadata.topic', match='rag'),\")\n",
    "print(\"    FieldCondition(key='metadata.difficulty', match='intermediate')\")\n",
    "print(\"  ]\")\n",
    "print(\"\\nğŸ’¡ Both conditions must be true (AND logic)\")\n",
    "print(\"ğŸ’¡ For OR logic, use should=[...] instead of must=[...]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Option 2: Qdrant with Local Persistence\n",
    "\n",
    "### When to Use\n",
    "- Data that needs to survive program restarts\n",
    "- Building a document index you'll reuse\n",
    "- Development/testing with persistent data\n",
    "\n",
    "### How It Works\n",
    "1. Create a client with a file path (e.g., `\"./qdrant_data\"`)\n",
    "2. Qdrant stores data in that directory\n",
    "3. Data persists between runs\n",
    "4. Can be version controlled (just commit the directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QDRANT WITH LOCAL PERSISTENCE\n",
      "================================================================================\n",
      "\n",
      "âœ“ Added documents to Qdrant (persistent)\n",
      "  Storage location: ./qdrant_data\n",
      "  Collection: my_collection_persistent\n",
      "  âš ï¸  Data will persist even after this script ends!\n",
      "\n",
      "Query: 'Tell me about LangChain'\n",
      "\n",
      "Search results:\n",
      "  1. LangChain simplifies LLM applications\n",
      "     Metadata: {'topic': 'langchain', 'difficulty': 'beginner', '_id': '65d1c23f76384cbe8085c822e7db4cab', '_collection_name': 'my_collection_persistent'}\n",
      "  2. RAG combines retrieval and generation\n",
      "     Metadata: {'topic': 'rag', 'difficulty': 'intermediate', '_id': '313825e613b64795ad07220fa26a750b', '_collection_name': 'my_collection_persistent'}\n",
      "\n",
      "ğŸ’¡ Next time you run this, you can load the same data from disk!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cj/13vbmk7n7fqgmdnqjjwn1bx80000gn/T/ipykernel_35525/1087051481.py:14: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant_client_persistent.recreate_collection(\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QDRANT WITH LOCAL PERSISTENCE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Step 1: Specify a local directory for storage\n",
    "qdrant_path = \"./qdrant_data\"\n",
    "\n",
    "# Step 2: Create persistent Qdrant client\n",
    "# Data will be saved in the ./qdrant_data directory\n",
    "qdrant_client_persistent = QdrantClient(path=qdrant_path)\n",
    "\n",
    "# Step 3: Create collection (same as before)\n",
    "qdrant_client_persistent.recreate_collection(\n",
    "    collection_name=\"my_collection_persistent\",\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "# Step 4: Create vector store wrapper\n",
    "qdrant_store_persistent = QdrantVectorStore(\n",
    "    client=qdrant_client_persistent,\n",
    "    collection_name=\"my_collection_persistent\",\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Step 5: Add documents\n",
    "qdrant_store_persistent.add_documents(sample_docs)\n",
    "print(f\"âœ“ Added documents to Qdrant (persistent)\")\n",
    "print(f\"  Storage location: {qdrant_path}\")\n",
    "print(f\"  Collection: my_collection_persistent\")\n",
    "print(f\"  âš ï¸  Data will persist even after this script ends!\")\n",
    "\n",
    "# Step 6: Search\n",
    "results = qdrant_store_persistent.similarity_search(\n",
    "    \"Tell me about LangChain\",\n",
    "    k=2\n",
    ")\n",
    "\n",
    "print(\"\\nQuery: 'Tell me about LangChain'\")\n",
    "print(\"\\nSearch results:\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"  {i}. {doc.page_content}\")\n",
    "    print(f\"     Metadata: {doc.metadata}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Next time you run this, you can load the same data from disk!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Option 3: Qdrant from_documents (Recommended â­)\n",
    "\n",
    "### Why This is Recommended\n",
    "- âœ… Simplest syntax (one method call)\n",
    "- âœ… Handles collection creation automatically\n",
    "- âœ… Supports persistence with just a `path` parameter\n",
    "- âœ… Returns a ready-to-use vector store\n",
    "\n",
    "### When to Use\n",
    "- Starting a new project\n",
    "- Quick prototyping\n",
    "- Most common use cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "QDRANT FROM_DOCUMENTS (RECOMMENDED METHOD)\n",
      "================================================================================\n",
      "\n",
      "âœ“ Created Qdrant store from documents\n",
      "  Collection: rag_collection\n",
      "  Storage: ./qdrant_easy\n",
      "  Documents: 3\n",
      "\n",
      "ğŸ’¡ This is the recommended approach for most use cases!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"QDRANT FROM_DOCUMENTS (RECOMMENDED METHOD)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Create Qdrant store directly from documents\n",
    "# This is the easiest way - everything happens in one call!\n",
    "qdrant_store_easy = QdrantVectorStore.from_documents(\n",
    "    documents=sample_docs,          # Your documents\n",
    "    embedding=embeddings,            # Embedding function\n",
    "    path=\"./qdrant_easy\",           # Local persistence (optional)\n",
    "    collection_name=\"rag_collection\" # Collection name\n",
    ")\n",
    "\n",
    "print(\"âœ“ Created Qdrant store from documents\")\n",
    "print(\"  Collection: rag_collection\")\n",
    "print(\"  Storage: ./qdrant_easy\")\n",
    "print(\"  Documents: 3\")\n",
    "print(\"\\nğŸ’¡ This is the recommended approach for most use cases!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search with Scores\n",
    "\n",
    "Sometimes you want to know **how similar** each result is. Use `similarity_search_with_score()` to get scores.\n",
    "\n",
    "**Score Interpretation:**\n",
    "- **Higher scores** = More similar\n",
    "- **COSINE distance:** Ranges from -1 to 1 (1 = identical, 0 = orthogonal, -1 = opposite)\n",
    "- In practice, scores > 0.7 are considered very similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'Vector databases'\n",
      "\n",
      "Search results with similarity scores:\n",
      "\n",
      "  Score: 0.7916\n",
      "  Content: Vector databases enable semantic search\n",
      "  Metadata: {'topic': 'vectordb', 'difficulty': 'intermediate', '_id': '6330528f78a04aa99589a34de8da1569', '_collection_name': 'rag_collection'}\n",
      "\n",
      "  Score: 0.4856\n",
      "  Content: RAG combines retrieval and generation\n",
      "  Metadata: {'topic': 'rag', 'difficulty': 'intermediate', '_id': '08a68459726f43dd90b831be1e6d6a9a', '_collection_name': 'rag_collection'}\n",
      "\n",
      "  Score: 0.3989\n",
      "  Content: LangChain simplifies LLM applications\n",
      "  Metadata: {'topic': 'langchain', 'difficulty': 'beginner', '_id': '7c713847180c49edafb6859e0f06f81c', '_collection_name': 'rag_collection'}\n",
      "\n",
      "ğŸ’¡ Scores help you filter out low-quality results\n",
      "ğŸ’¡ You can set a threshold (e.g., only return results with score > 0.7)\n"
     ]
    }
   ],
   "source": [
    "# Search with scores\n",
    "results_with_scores = qdrant_store_easy.similarity_search_with_score(\n",
    "    \"Vector databases\",\n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(\"\\nQuery: 'Vector databases'\")\n",
    "print(\"\\nSearch results with similarity scores:\")\n",
    "print()\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"  Score: {score:.4f}\")  # Similarity score (higher = more similar)\n",
    "    print(f\"  Content: {doc.page_content}\")\n",
    "    print(f\"  Metadata: {doc.metadata}\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸ’¡ Scores help you filter out low-quality results\")\n",
    "print(\"ğŸ’¡ You can set a threshold (e.g., only return results with score > 0.7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ—„ï¸ Part 2: Weaviate Vector Store\n",
    "\n",
    "## What is Weaviate?\n",
    "\n",
    "**Weaviate** is a production-ready vector database that offers:\n",
    "- âœ… Excellent scalability (handles millions of vectors)\n",
    "- âœ… Advanced features (hybrid search, multi-tenancy)\n",
    "- âœ… GraphQL API\n",
    "- âœ… Built for production environments\n",
    "\n",
    "## When to Use Weaviate\n",
    "\n",
    "âœ… **Use Weaviate when:**\n",
    "- Building production applications\n",
    "- Need to scale to large datasets (millions of documents)\n",
    "- Want advanced features like hybrid search\n",
    "- Have Docker available\n",
    "\n",
    "âŒ **Don't use Weaviate when:**\n",
    "- Doing quick prototypes (use Qdrant instead)\n",
    "- Can't use Docker\n",
    "- Working with small datasets (< 10k documents)\n",
    "\n",
    "## Setup Requirements\n",
    "\n",
    "Weaviate requires Docker to run locally:\n",
    "```bash\n",
    "docker run -d -p 8080:8080 -p 50051:50051 \\\n",
    "  --name weaviate \\\n",
    "  cr.weaviate.io/semitechnologies/weaviate:latest\n",
    "```\n",
    "\n",
    "## Filter Syntax Difference\n",
    "\n",
    "Weaviate uses `where_filter` with a different syntax than Qdrant:\n",
    "\n",
    "```python\n",
    "where_filter={\n",
    "    \"path\": [\"difficulty\"],\n",
    "    \"operator\": \"Equal\",\n",
    "    \"valueText\": \"intermediate\"\n",
    "}\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WEAVIATE LOCAL VECTOR STORE EXAMPLE\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  Note: This requires Weaviate running locally on port 8080\n",
      "   If not running, you'll see connection errors (that's OK for learning!)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Connecting to Local Weaviate\n",
      "--------------------------------------------------------------------------------\n",
      "âœ“ Connected to local Weaviate\n",
      "  Host: localhost:8080\n",
      "  gRPC Port: 50051\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Creating Weaviate Vector Store\n",
      "--------------------------------------------------------------------------------\n",
      "âœ“ Added documents to Weaviate\n",
      "  Index: MyDocuments\n",
      "  Documents: 3\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Basic Search\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Tell me about RAG'\n",
      "\n",
      "Search results:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Metadata: {'difficulty': 'intermediate', 'topic': 'rag'}\n",
      "  2. RAG combines retrieval and generation\n",
      "     Metadata: {'difficulty': 'intermediate', 'topic': 'rag'}\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Search with Metadata Filter\n",
      "--------------------------------------------------------------------------------\n",
      "âœ— Weaviate error: _HybridQueryExecutor.hybrid() got an unexpected keyword argument 'where_filter'\n",
      "\n",
      "Troubleshooting:\n",
      "1. Check if Weaviate is running: docker ps\n",
      "2. Start Weaviate: docker run -d -p 8080:8080 -p 50051:50051 \\\n",
      "     --name weaviate cr.weaviate.io/semitechnologies/weaviate:latest\n",
      "3. Check if port 8080 is available: lsof -i :8080\n",
      "4. Check Weaviate logs: docker logs weaviate\n",
      "\n",
      "ğŸ’¡ It's OK if this doesn't work - you can still learn from the code!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"WEAVIATE LOCAL VECTOR STORE EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"âš ï¸  Note: This requires Weaviate running locally on port 8080\")\n",
    "print(\"   If not running, you'll see connection errors (that's OK for learning!)\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    import weaviate\n",
    "    from langchain_weaviate import WeaviateVectorStore\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"Connecting to Local Weaviate\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Step 1: Connect to local Weaviate instance\n",
    "    # This assumes Weaviate is running on localhost:8080\n",
    "    weaviate_client = weaviate.connect_to_local(\n",
    "        host=\"localhost\",\n",
    "        port=8080,\n",
    "        grpc_port=50051\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ Connected to local Weaviate\")\n",
    "    print(\"  Host: localhost:8080\")\n",
    "    print(\"  gRPC Port: 50051\")\n",
    "    \n",
    "    # Step 2: Create Weaviate vector store\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Creating Weaviate Vector Store\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    weaviate_store = WeaviateVectorStore(\n",
    "        client=weaviate_client,\n",
    "        index_name=\"MyDocuments\",  # Collection name in Weaviate\n",
    "        text_key=\"text\",            # Field name for document text\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # Step 3: Add documents\n",
    "    weaviate_store.add_documents(sample_docs)\n",
    "    print(\"âœ“ Added documents to Weaviate\")\n",
    "    print(\"  Index: MyDocuments\")\n",
    "    print(\"  Documents: 3\")\n",
    "    \n",
    "    # Step 4: Basic Search\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Basic Search\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    results = weaviate_store.similarity_search(\n",
    "        \"Tell me about RAG\",\n",
    "        k=2\n",
    "    )\n",
    "    \n",
    "    print(\"\\nQuery: 'Tell me about RAG'\")\n",
    "    print(\"\\nSearch results:\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"  {i}. {doc.page_content}\")\n",
    "        print(f\"     Metadata: {doc.metadata}\")\n",
    "    \n",
    "    # Step 5: Search with Metadata Filter\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Search with Metadata Filter\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Weaviate uses where_filter with different syntax\n",
    "    results_filtered = weaviate_store.similarity_search(\n",
    "        \"Tell me about databases\",\n",
    "        k=2,\n",
    "        where_filter={\n",
    "            \"path\": [\"difficulty\"],      # Metadata field to filter on\n",
    "            \"operator\": \"Equal\",          # Comparison operator\n",
    "            \"valueText\": \"intermediate\"   # Value to match\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"\\nQuery: 'Tell me about databases'\")\n",
    "    print(\"Filter: difficulty='intermediate'\")\n",
    "    print(\"\\nFiltered search results:\")\n",
    "    for i, doc in enumerate(results_filtered, 1):\n",
    "        print(f\"  {i}. {doc.page_content}\")\n",
    "        print(f\"     Metadata: {doc.metadata}\")\n",
    "    \n",
    "    # Step 6: Search with Scores\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Search with Scores\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    results_with_scores = weaviate_store.similarity_search_with_score(\n",
    "        \"Vector databases\",\n",
    "        k=3\n",
    "    )\n",
    "    \n",
    "    print(\"\\nQuery: 'Vector databases'\")\n",
    "    print(\"\\nSearch results with scores:\")\n",
    "    for doc, score in results_with_scores:\n",
    "        print(f\"  Score: {score:.4f}\")\n",
    "        print(f\"  Content: {doc.page_content}\")\n",
    "        print(f\"  Metadata: {doc.metadata}\")\n",
    "        print()\n",
    "    \n",
    "    # Step 7: Alternative - Create from Documents\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Creating Weaviate from Documents (Alternative Method)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    weaviate_store_easy = WeaviateVectorStore.from_documents(\n",
    "        documents=sample_docs,\n",
    "        embedding=embeddings,\n",
    "        client=weaviate_client,\n",
    "        index_name=\"EasyDocuments\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ Created Weaviate store from documents\")\n",
    "    \n",
    "    # Quick search\n",
    "    results = weaviate_store_easy.similarity_search(\"LangChain\", k=2)\n",
    "    print(\"\\nQuick search results:\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"  {i}. {doc.page_content}\")\n",
    "    \n",
    "    # Clean up\n",
    "    weaviate_client.close()\n",
    "    print(\"\\nâœ“ Closed Weaviate connection\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Weaviate error: {e}\")\n",
    "    print()\n",
    "    print(\"Troubleshooting:\")\n",
    "    print(\"1. Check if Weaviate is running: docker ps\")\n",
    "    print(\"2. Start Weaviate: docker run -d -p 8080:8080 -p 50051:50051 \\\\\")\n",
    "    print(\"     --name weaviate cr.weaviate.io/semitechnologies/weaviate:latest\")\n",
    "    print(\"3. Check if port 8080 is available: lsof -i :8080\")\n",
    "    print(\"4. Check Weaviate logs: docker logs weaviate\")\n",
    "    print()\n",
    "    print(\"ğŸ’¡ It's OK if this doesn't work - you can still learn from the code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸš€ Part 3: Real Ollama Embeddings Integration\n",
    "\n",
    "## Complete Working Example\n",
    "\n",
    "This section demonstrates a complete, production-ready setup with:\n",
    "- âœ… Real Ollama embeddings (not dummy/fake embeddings)\n",
    "- âœ… Qdrant for storage\n",
    "- âœ… Ready to integrate into RAG pipelines\n",
    "\n",
    "## Testing Your Ollama Connection\n",
    "\n",
    "First, let's verify that Ollama is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BONUS: REAL OLLAMA EMBEDDINGS EXAMPLE\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Testing Ollama Connection\n",
      "--------------------------------------------------------------------------------\n",
      "âœ“ Ollama embeddings working!\n",
      "  Model: nomic-embed-text\n",
      "  Embedding dimension: 768\n",
      "  Sample values (first 5): ['0.0496', '0.0668', '-0.1677', '-0.0624', '0.0201']\n",
      "\n",
      "ğŸ’¡ Each document is converted to 768 numbers that capture its meaning!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Creating Qdrant with Ollama Embeddings\n",
      "--------------------------------------------------------------------------------\n",
      "âœ“ Created Qdrant store with Ollama embeddings\n",
      "  Storage: ./qdrant_ollama\n",
      "  Collection: ollama_collection\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Searching with Real Embeddings\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: 'Tell me about RAG'\n",
      "\n",
      "Results:\n",
      "  1. RAG combines retrieval and generation\n",
      "     Topic: rag\n",
      "     Difficulty: intermediate\n",
      "  2. Vector databases enable semantic search\n",
      "     Topic: vectordb\n",
      "     Difficulty: intermediate\n",
      "\n",
      "âœ“ Ollama integration successful!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Using in RAG Pipeline\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "To use this vector store in a RAG pipeline:\n",
      "\n",
      "  # Convert to retriever\n",
      "  retriever = ollama_qdrant_store.as_retriever(\n",
      "      search_kwargs={'k': 4}  # Return top 4 results\n",
      "  )\n",
      "\n",
      "  # Use in RAG chain\n",
      "  from langchain_core.runnables import RunnablePassthrough\n",
      "  from langchain_core.output_parsers import StrOutputParser\n",
      "\n",
      "  rag_chain = (\n",
      "      {'context': retriever, 'question': RunnablePassthrough()}\n",
      "      | prompt\n",
      "      | llm\n",
      "      | StrOutputParser()\n",
      "  )\n",
      "\n",
      "ğŸ’¡ The retriever fetches relevant docs, then the LLM generates an answer!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BONUS: REAL OLLAMA EMBEDDINGS EXAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "try:\n",
    "    from langchain_ollama import OllamaEmbeddings\n",
    "    from langchain_qdrant import QdrantVectorStore\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(\"Testing Ollama Connection\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Initialize Ollama embeddings\n",
    "    ollama_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    \n",
    "    # Test embeddings by converting a sample text\n",
    "    test_text = \"This is a test with real Ollama embeddings\"\n",
    "    test_embedding = ollama_embeddings.embed_query(test_text)\n",
    "    \n",
    "    print(f\"âœ“ Ollama embeddings working!\")\n",
    "    print(f\"  Model: nomic-embed-text\")\n",
    "    print(f\"  Embedding dimension: {len(test_embedding)}\")\n",
    "    print(f\"  Sample values (first 5): {[f'{x:.4f}' for x in test_embedding[:5]]}\")\n",
    "    print()\n",
    "    print(\"ğŸ’¡ Each document is converted to 768 numbers that capture its meaning!\")\n",
    "    \n",
    "    # Create Qdrant store with real embeddings\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Creating Qdrant with Ollama Embeddings\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    ollama_qdrant_store = QdrantVectorStore.from_documents(\n",
    "        documents=sample_docs,\n",
    "        embedding=ollama_embeddings,\n",
    "        path=\"./qdrant_ollama\",\n",
    "        collection_name=\"ollama_collection\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ Created Qdrant store with Ollama embeddings\")\n",
    "    print(\"  Storage: ./qdrant_ollama\")\n",
    "    print(\"  Collection: ollama_collection\")\n",
    "    \n",
    "    # Search with real embeddings\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Searching with Real Embeddings\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    query = \"Tell me about RAG\"\n",
    "    results = ollama_qdrant_store.similarity_search(query, k=2)\n",
    "    \n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(\"\\nResults:\")\n",
    "    for i, doc in enumerate(results, 1):\n",
    "        print(f\"  {i}. {doc.page_content}\")\n",
    "        print(f\"     Topic: {doc.metadata.get('topic')}\")\n",
    "        print(f\"     Difficulty: {doc.metadata.get('difficulty')}\")\n",
    "    \n",
    "    print(\"\\nâœ“ Ollama integration successful!\")\n",
    "    \n",
    "    # Show how to use in RAG\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"Using in RAG Pipeline\")\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    print(\"To use this vector store in a RAG pipeline:\")\n",
    "    print()\n",
    "    print(\"  # Convert to retriever\")\n",
    "    print(\"  retriever = ollama_qdrant_store.as_retriever(\")\n",
    "    print(\"      search_kwargs={'k': 4}  # Return top 4 results\")\n",
    "    print(\"  )\")\n",
    "    print()\n",
    "    print(\"  # Use in RAG chain\")\n",
    "    print(\"  from langchain_core.runnables import RunnablePassthrough\")\n",
    "    print(\"  from langchain_core.output_parsers import StrOutputParser\")\n",
    "    print()\n",
    "    print(\"  rag_chain = (\")\n",
    "    print(\"      {'context': retriever, 'question': RunnablePassthrough()}\")\n",
    "    print(\"      | prompt\")\n",
    "    print(\"      | llm\")\n",
    "    print(\"      | StrOutputParser()\")\n",
    "    print(\"  )\")\n",
    "    print()\n",
    "    print(\"ğŸ’¡ The retriever fetches relevant docs, then the LLM generates an answer!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âœ— langchain-ollama not installed\")\n",
    "    print()\n",
    "    print(\"Install with:\")\n",
    "    print(\"  pip install langchain-ollama\")\n",
    "    print()\n",
    "    print(\"Example code to use when installed:\")\n",
    "    print()\n",
    "    print(\"from langchain_ollama import OllamaEmbeddings\")\n",
    "    print(\"from langchain_qdrant import QdrantVectorStore\")\n",
    "    print()\n",
    "    print(\"# Initialize embeddings\")\n",
    "    print(\"embeddings = OllamaEmbeddings(model='nomic-embed-text')\")\n",
    "    print()\n",
    "    print(\"# Create vector store\")\n",
    "    print(\"qdrant_store = QdrantVectorStore.from_documents(\")\n",
    "    print(\"    documents=your_documents,\")\n",
    "    print(\"    embedding=embeddings,\")\n",
    "    print(\"    path='./qdrant_local',\")\n",
    "    print(\"    collection_name='my_collection'\")\n",
    "    print(\")\")\n",
    "    print()\n",
    "    print(\"# Use as retriever in RAG\")\n",
    "    print(\"retriever = qdrant_store.as_retriever(search_kwargs={'k': 4})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error with Ollama: {e}\")\n",
    "    print()\n",
    "    print(\"Troubleshooting:\")\n",
    "    print(\"1. Make sure Ollama is running:\")\n",
    "    print(\"   ollama serve\")\n",
    "    print()\n",
    "    print(\"2. Make sure the model is downloaded:\")\n",
    "    print(\"   ollama pull nomic-embed-text\")\n",
    "    print()\n",
    "    print(\"3. Make sure langchain-ollama is installed:\")\n",
    "    print(\"   pip install langchain-ollama\")\n",
    "    print()\n",
    "    print(\"4. Check Ollama is accessible:\")\n",
    "    print(\"   curl http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“Š Vector Store Comparison\n",
    "\n",
    "## Feature Comparison Table\n",
    "\n",
    "| Feature | Qdrant | Weaviate | ChromaDB |\n",
    "|---------|--------|----------|----------|\n",
    "| **Setup** | Easy (no Docker) | Requires Docker | Easiest (pip install) |\n",
    "| **Persistence** | Local file/in-memory | Docker volume | Local file/in-memory |\n",
    "| **Performance** | Fast | Very fast | Fast |\n",
    "| **Scalability** | Good (millions) | Excellent (billions) | Good (thousands) |\n",
    "| **Filter Syntax** | Filter objects | where_filter dict | Simple dict |\n",
    "| **Memory Usage** | Low | Medium | Low |\n",
    "| **Production Ready** | Yes | Yes | Limited |\n",
    "| **Best For** | Local development | Production apps | Quick prototypes |\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "### Use **Qdrant** when:\n",
    "- ğŸ¯ Local development and testing\n",
    "- ğŸ¯ Don't want to manage Docker\n",
    "- ğŸ¯ Need good performance for small-to-medium datasets\n",
    "- ğŸ¯ Want file-based persistence\n",
    "\n",
    "### Use **Weaviate** when:\n",
    "- ğŸ¯ Building production applications\n",
    "- ğŸ¯ Need to scale to large datasets (millions of vectors)\n",
    "- ğŸ¯ Want advanced features (GraphQL, hybrid search)\n",
    "- ğŸ¯ Have Docker infrastructure\n",
    "\n",
    "### Use **ChromaDB** when:\n",
    "- ğŸ¯ Quick prototyping and demos\n",
    "- ğŸ¯ Absolute simplest setup\n",
    "- ğŸ¯ Small datasets (< 10k documents)\n",
    "- ğŸ¯ Learning vector databases\n",
    "\n",
    "## Performance Considerations\n",
    "\n",
    "### Dataset Size Guidelines\n",
    "- **Small (< 10k docs):** Any vector store works\n",
    "- **Medium (10k - 100k docs):** Qdrant or Weaviate\n",
    "- **Large (100k - 1M+ docs):** Weaviate or managed services\n",
    "\n",
    "### Memory Usage\n",
    "- **In-memory:** Fast but limited by RAM\n",
    "- **Persistent:** Slower but can handle larger datasets\n",
    "- **Rule of thumb:** Budget ~1KB per document + embedding size\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ“ Summary and Next Steps\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "### 1. Vector Store Basics\n",
    "- âœ… What vector stores are and why they're essential for RAG\n",
    "- âœ… How embeddings convert text to numbers that capture meaning\n",
    "- âœ… How similarity search finds relevant documents\n",
    "\n",
    "### 2. Qdrant\n",
    "- âœ… Three ways to use Qdrant (in-memory, persistent, from_documents)\n",
    "- âœ… Basic similarity search\n",
    "- âœ… Metadata filtering with Filter objects\n",
    "- âœ… Multiple filter conditions (AND logic)\n",
    "- âœ… Similarity search with scores\n",
    "\n",
    "### 3. Weaviate\n",
    "- âœ… Docker-based setup\n",
    "- âœ… Different filter syntax (where_filter)\n",
    "- âœ… When to use Weaviate vs Qdrant\n",
    "\n",
    "### 4. Ollama Integration\n",
    "- âœ… Real embeddings with nomic-embed-text\n",
    "- âœ… Complete working example\n",
    "- âœ… How to use in RAG pipelines\n",
    "\n",
    "### 5. Best Practices\n",
    "- âœ… Correct LangChain 1.0+ imports\n",
    "- âœ… When to use each vector store\n",
    "- âœ… How to handle errors and troubleshooting\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Start with Qdrant** for local development (no Docker needed)\n",
    "2. **Use `from_documents()`** for the simplest setup\n",
    "3. **Always use correct imports**: `from langchain_core.documents import Document`\n",
    "4. **Filter syntax varies** between vector stores (learn each one)\n",
    "5. **Ollama provides free, local embeddings** (great for development)\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### 1. Practice Exercises\n",
    "Try these on your own:\n",
    "\n",
    "**Exercise 1:** Create your own documents\n",
    "- Create 5-10 documents about a topic you know\n",
    "- Add meaningful metadata\n",
    "- Test different search queries\n",
    "\n",
    "**Exercise 2:** Experiment with filters\n",
    "- Create documents with multiple metadata fields\n",
    "- Try different filter combinations\n",
    "- Compare results with and without filters\n",
    "\n",
    "**Exercise 3:** Compare vector stores\n",
    "- Use the same documents in Qdrant and Weaviate\n",
    "- Search for the same queries\n",
    "- Compare the results and performance\n",
    "\n",
    "**Exercise 4:** Build a simple RAG system\n",
    "- Load documents from a file\n",
    "- Create a vector store\n",
    "- Build a retriever\n",
    "- Integrate with an LLM\n",
    "\n",
    "### 2. Explore More Features\n",
    "- **Hybrid search** (combining keyword and semantic search)\n",
    "- **MMR (Maximal Marginal Relevance)** for diverse results\n",
    "- **Compression retrievers** for better context\n",
    "- **Multi-query retrievers** for comprehensive retrieval\n",
    "\n",
    "### 3. Resources for Learning\n",
    "\n",
    "**Official Documentation:**\n",
    "- [Qdrant Documentation](https://qdrant.tech/documentation/)\n",
    "- [Weaviate Documentation](https://weaviate.io/developers/weaviate)\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/)\n",
    "- [Ollama Documentation](https://ollama.ai/)\n",
    "\n",
    "**LangChain Tutorials:**\n",
    "- [Vector Stores Guide](https://python.langchain.com/docs/modules/data_connection/vectorstores/)\n",
    "- [RAG Tutorial](https://python.langchain.com/docs/use_cases/question_answering/)\n",
    "- [Retrievers Guide](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
    "\n",
    "### 4. Build Your Own Project\n",
    "\n",
    "Apply what you've learned:\n",
    "- **Personal knowledge base** - Index your notes and documents\n",
    "- **Customer support bot** - Search company documentation\n",
    "- **Code search** - Index and search code repositories\n",
    "- **Research assistant** - Search academic papers\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You now have a solid foundation in vector stores and are ready to build RAG applications!\n",
    "\n",
    "### Remember:\n",
    "- Start simple (Qdrant + from_documents)\n",
    "- Use real embeddings (Ollama)\n",
    "- Test with small datasets first\n",
    "- Scale up as needed\n",
    "\n",
    "### Happy Building! ğŸš€\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
