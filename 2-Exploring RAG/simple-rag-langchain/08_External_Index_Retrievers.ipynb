{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - External Index Retrievers üåê\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "\n",
    "1. **What are External Index Retrievers** and how they differ from vector store retrievers\n",
    "2. **ArxivRetriever** - Search and retrieve scholarly articles from arxiv.org\n",
    "3. **WikipediaRetriever** - Access Wikipedia articles for general knowledge\n",
    "4. **TavilySearchAPIRetriever** - Perform real-time internet searches\n",
    "5. **Integration with RAG Chains** - Combine external retrievers with LLMs\n",
    "6. **Best Practices** - When and how to use each retriever effectively\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents üìö\n",
    "\n",
    "1. [Introduction to External Retrievers](#intro)\n",
    "2. [Setup & Installation](#setup)\n",
    "3. [ArxivRetriever - Academic Papers](#arxiv)\n",
    "4. [WikipediaRetriever - General Knowledge](#wikipedia)\n",
    "5. [TavilySearchAPIRetriever - Web Search](#tavily)\n",
    "6. [Integration with RAG Chains](#rag)\n",
    "7. [Comparison & Use Cases](#comparison)\n",
    "8. [Best Practices](#best-practices)\n",
    "9. [Summary & Exercises](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction to External Index Retrievers üîç\n",
    "\n",
    "### What are External Index Retrievers?\n",
    "\n",
    "**External Index Retrievers** search over external data sources (e.g., the internet, academic databases, knowledge bases) rather than your local vector store.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature | Vector Store Retrievers | External Index Retrievers |\n",
    "|---------|------------------------|---------------------------|\n",
    "| **Data Source** | Your embedded documents | External databases/APIs |\n",
    "| **Data Freshness** | Static (at indexing time) | Real-time or regularly updated |\n",
    "| **Setup Required** | Embedding + Vector store | API keys (sometimes) |\n",
    "| **Use Cases** | Internal documents, knowledge bases | Current events, academic research, general knowledge |\n",
    "| **Cost** | Embedding cost + storage | API calls (often free tier available) |\n",
    "\n",
    "### When to Use External Retrievers:\n",
    "\n",
    "- ‚úÖ You need **up-to-date information** from the internet\n",
    "- ‚úÖ You want to access **specialized databases** (e.g., academic papers)\n",
    "- ‚úÖ You need **general knowledge** without building a custom knowledge base\n",
    "- ‚úÖ You want to **augment** your local data with external sources\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2. Setup & Installation ‚öôÔ∏è\n",
    "\n",
    "### Required Packages\n",
    "\n",
    "All external retrievers are part of `langchain-community`. You'll also need:\n",
    "\n",
    "```bash\n",
    "pip install langchain-community\n",
    "pip install arxiv           # For ArxivRetriever\n",
    "pip install wikipedia       # For WikipediaRetriever\n",
    "pip install tavily-python   # For TavilySearchAPIRetriever\n",
    "```\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "For TavilySearchAPIRetriever, you'll need an API key:\n",
    "\n",
    "```\n",
    "TAVILY_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "Get your free API key at: https://tavily.com/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain version: 0.3.27\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_community.retrievers import ArxivRetriever, WikipediaRetriever, TavilySearchAPIRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Verify versions\n",
    "import langchain\n",
    "print(f\"‚úÖ LangChain version: {langchain.__version__}\")\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arxiv'></a>\n",
    "## 3. ArxivRetriever - Academic Papers üìÑ\n",
    "\n",
    "### üî∞ BEGINNER: What is ArxivRetriever?\n",
    "\n",
    "**ArxivRetriever** searches [arxiv.org](https://arxiv.org), a repository of electronic preprints for research papers in:\n",
    "- Physics\n",
    "- Mathematics\n",
    "- Computer Science\n",
    "- Quantitative Biology\n",
    "- Quantitative Finance\n",
    "- Statistics\n",
    "\n",
    "### Use Cases:\n",
    "- üìö Literature review for research\n",
    "- üß† Getting latest research on AI/ML topics\n",
    "- üìä Finding papers by specific authors\n",
    "- üî¨ Accessing cutting-edge research\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic ArxivRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m8 packages\u001b[0m \u001b[2min 915ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 41ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1marxiv\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeedparser\u001b[0m\u001b[2m==6.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msgmllib3k\u001b[0m\u001b[2m==1.0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 3 papers on 'large language models'\n",
      "\n",
      "================================================================================\n",
      "Title: Large Language Models Lack Understanding of Character Composition of Words\n",
      "Authors: Andrew Shin, Kunitake Kaneko\n",
      "Published: 2024-07-23\n",
      "\n",
      "Abstract (first 500 chars):\n",
      "Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple t...\n",
      "================================================================================\n",
      "Title: Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality\n",
      "================================================================================\n",
      "Title: Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models\n"
     ]
    }
   ],
   "source": [
    "# Create an ArxivRetriever instance\n",
    "# By default, it returns top 3 documents\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=3)\n",
    "\n",
    "# Search for papers on \"large language models\"\n",
    "query = \"large language models\"\n",
    "docs = arxiv_retriever.invoke(query)\n",
    "\n",
    "print(f\"üìö Found {len(docs)} papers on '{query}'\\n\")\n",
    "\n",
    "# Display first paper\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[0].metadata.get('Title', 'N/A')}\")\n",
    "print(f\"Authors: {docs[0].metadata.get('Authors', 'N/A')}\")\n",
    "print(f\"Published: {docs[0].metadata.get('Published', 'N/A')}\")\n",
    "print(f\"\\nAbstract (first 500 chars):\\n{docs[0].page_content[:500]}...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[1].metadata.get('Title', 'N/A')}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[2].metadata.get('Title', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced ArxivRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Retrieved 3 papers\n",
      "\n",
      "1. Vision Transformer with Quadrangle Attention\n",
      "   Authors: Qiming Zhang, Jing Zhang, Yufei Xu, Dacheng Tao\n",
      "   Published: 2023-03-27\n",
      "   Entry ID: N/A\n",
      "\n",
      "2. D√©j√† vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation\n",
      "   Authors: Jibang Wu, Renqin Cai, Hongning Wang\n",
      "   Published: 2020-01-29\n",
      "   Entry ID: N/A\n",
      "\n",
      "3. Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture\n",
      "   Authors: Nihal Mehta\n",
      "   Published: 2025-11-16\n",
      "   Entry ID: N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Retrieve more documents and explore metadata\n",
    "arxiv_retriever_advanced = ArxivRetriever(\n",
    "    load_max_docs=5,  # Get top 5 papers\n",
    "    load_all_available_meta=True  # Load all metadata\n",
    ")\n",
    "\n",
    "# Search for papers on \"transformers attention mechanism\"\n",
    "query = \"transformers attention mechanism\"\n",
    "docs = arxiv_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üìö Retrieved {len(docs)} papers\\n\")\n",
    "\n",
    "# Display metadata for all papers\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. {doc.metadata.get('Title', 'N/A')}\")\n",
    "    print(f\"   Authors: {doc.metadata.get('Authors', 'N/A')}\")\n",
    "    print(f\"   Published: {doc.metadata.get('Published', 'N/A')}\")\n",
    "    print(f\"   Entry ID: {doc.metadata.get('entry_id', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Using .batch() for Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Batch Search Results:\n",
      "\n",
      "Query: 'RAG retrieval augmented generation'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: AR-RAG: Autoregressive Retrieval Augmentation for Image Generation\n",
      "\n",
      "Query: 'vector embeddings'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: Part-of-Speech Relevance Weights for Learning Word Embeddings\n",
      "\n",
      "Query: 'prompt engineering'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch processing: Search multiple topics at once\n",
    "queries = [\n",
    "    \"RAG retrieval augmented generation\",\n",
    "    \"vector embeddings\",\n",
    "    \"prompt engineering\"\n",
    "]\n",
    "\n",
    "arxiv_retriever_batch = ArxivRetriever(load_max_docs=3)\n",
    "batch_results = arxiv_retriever_batch.batch(queries)\n",
    "\n",
    "print(\"üìö Batch Search Results:\\n\")\n",
    "for query, docs in zip(queries, batch_results):\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"  ‚Üí Found {len(docs)} papers\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí Top result: {docs[0].metadata.get('Title', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding ArxivRetriever Metadata\n",
    "\n",
    "Each document returned by ArxivRetriever contains rich metadata:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Published': '2023-06-15',           # Publication date\n",
    "    'Title': 'Paper Title',              # Full title\n",
    "    'Authors': 'Author1, Author2',       # Comma-separated authors\n",
    "    'Summary': 'Abstract text...',       # Paper abstract/summary\n",
    "    'entry_id': 'http://arxiv.org/...',  # Arxiv URL\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the full abstract/summary of the paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wikipedia'></a>\n",
    "## 4. WikipediaRetriever - General Knowledge üìñ\n",
    "\n",
    "### üî∞ BEGINNER: What is WikipediaRetriever?\n",
    "\n",
    "**WikipediaRetriever** searches and retrieves content from Wikipedia, the free encyclopedia with 6+ million articles.\n",
    "\n",
    "### Use Cases:\n",
    "- üåç General knowledge questions\n",
    "- üìö Quick facts and definitions\n",
    "- üèõÔ∏è Historical information\n",
    "- üßë‚Äçüî¨ Biographical data\n",
    "- üó∫Ô∏è Geographic information\n",
    "\n",
    "### Important Notes:\n",
    "- ‚ö†Ô∏è Wikipedia content is **community-edited** - verify critical information\n",
    "- ‚úÖ Great for general knowledge, not for specialized or proprietary data\n",
    "- üåê Supports multiple languages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic WikipediaRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 1.21s\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 486ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwikipedia\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Found 2 Wikipedia articles on 'Python programming language'\n",
      "\n",
      "================================================================================\n",
      "Title: Python (programming language)\n",
      "Source: https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "\n",
      "Content (first 600 chars):\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.\n",
      "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language. Python 3.0, released in 2008, was a major revision and not completely backward-compatible with earlier versions. Beginning with Python 3.5, capabi...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a WikipediaRetriever instance\n",
    "# By default, it returns top 3 documents\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2)\n",
    "\n",
    "# Search for information on \"Python programming language\"\n",
    "query = \"Python programming language\"\n",
    "docs = wiki_retriever.invoke(query)\n",
    "\n",
    "print(f\"üìñ Found {len(docs)} Wikipedia articles on '{query}'\\n\")\n",
    "\n",
    "# Display first result\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "print(f\"\\nContent (first 600 chars):\\n{docs[0].page_content[:600]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced WikipediaRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Retrieved 3 Wikipedia articles\n",
      "\n",
      "1. Title: Machine learning\n",
      "   Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
      "   Content length: 1000 characters\n",
      "\n",
      "2. Title: Neural network (machine learning)\n",
      "   Summary: In machine learning, a neural network or neural net (NN), also called artificial neural network (ANN), is a computational model inspired by the struct...\n",
      "   Content length: 1000 characters\n",
      "\n",
      "3. Title: Attention (machine learning)\n",
      "   Summary: In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that seq...\n",
      "   Content length: 1000 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Control number of results and document length\n",
    "wiki_retriever_advanced = WikipediaRetriever(\n",
    "    top_k_results=3,        # Get top 3 results\n",
    "    doc_content_chars_max=1000  # Limit content to 1000 characters per doc\n",
    ")\n",
    "\n",
    "# Search for \"Machine Learning\"\n",
    "query = \"Machine Learning\"\n",
    "docs = wiki_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üìñ Retrieved {len(docs)} Wikipedia articles\\n\")\n",
    "\n",
    "# Display all results\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. Title: {doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"   Summary: {doc.metadata.get('summary', 'N/A')[:150]}...\")\n",
    "    print(f\"   Content length: {len(doc.page_content)} characters\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Multilingual Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Search in Spanish Wikipedia: 'Inteligencia Artificial'\n",
      "\n",
      "Title: Inteligencia artificial\n",
      "Content preview:\n",
      "La inteligencia artificial, abreviado como IA, en el contexto de las ciencias de la computaci√≥n, es una disciplina y un conjunto de capacidades cognoscitivas e intelectuales expresadas por sistemas inform√°ticos o combinaciones de algoritmos cuyo prop√≥sito es la creaci√≥n de m√°quinas que imiten la inteligencia humana.\n",
      "Estas tecnolog√≠as permiten que las m√°quinas aprendan de la experiencia, se adapten...\n"
     ]
    }
   ],
   "source": [
    "# Search in different languages\n",
    "# Default is English ('en'), but you can specify other languages\n",
    "\n",
    "# Example: Search in Spanish\n",
    "wiki_retriever_es = WikipediaRetriever(\n",
    "    top_k_results=1,\n",
    "    lang=\"es\"  # Spanish Wikipedia\n",
    ")\n",
    "\n",
    "query = \"Inteligencia Artificial\"\n",
    "docs = wiki_retriever_es.invoke(query)\n",
    "\n",
    "print(f\"üåê Search in Spanish Wikipedia: '{query}'\\n\")\n",
    "print(f\"Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "print(f\"Content preview:\\n{docs[0].page_content[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Batch Processing with WikipediaRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Batch Wikipedia Search Results:\n",
      "\n",
      "Query: 'Albert Einstein'\n",
      "  ‚Üí Title: Albert Einstein\n",
      "  ‚Üí Summary: Albert Einstein (14 March 1879 ‚Äì 18 April 1955) was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory...\n",
      "\n",
      "Query: 'Quantum Computing'\n",
      "  ‚Üí Title: Quantum computing\n",
      "  ‚Üí Summary: A quantum computer is a (real or theoretical) computer that exploits superposed and entangled states, and the intrinsically non-deterministic outcomes of quantum measurements, as features of its compu...\n",
      "\n",
      "Query: 'Neural Networks'\n",
      "  ‚Üí Title: Neural network (machine learning)\n",
      "  ‚Üí Summary: In machine learning, a neural network or neural net (NN), also called artificial neural network (ANN), is a computational model inspired by the structure and functions of biological neural networks.\n",
      "A...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch search for multiple topics\n",
    "queries = [\n",
    "    \"Albert Einstein\",\n",
    "    \"Quantum Computing\",\n",
    "    \"Neural Networks\"\n",
    "]\n",
    "\n",
    "wiki_retriever_batch = WikipediaRetriever(top_k_results=1, doc_content_chars_max=500)\n",
    "batch_results = wiki_retriever_batch.batch(queries)\n",
    "\n",
    "print(\"üìñ Batch Wikipedia Search Results:\\n\")\n",
    "for query, docs in zip(queries, batch_results):\n",
    "    print(f\"Query: '{query}'\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "        print(f\"  ‚Üí Summary: {docs[0].page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding WikipediaRetriever Metadata\n",
    "\n",
    "Each document returned by WikipediaRetriever contains:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'title': 'Article Title',           # Wikipedia article title\n",
    "    'summary': 'Brief summary...',       # Short summary (if available)\n",
    "    'source': 'https://en.wikipedia...', # Full Wikipedia URL\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the article text (up to `doc_content_chars_max` characters).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tavily'></a>\n",
    "## 5. TavilySearchAPIRetriever - Web Search üîç\n",
    "\n",
    "### üî∞ BEGINNER: What is TavilySearchAPIRetriever?\n",
    "\n",
    "**TavilySearchAPIRetriever** performs **real-time internet searches** using the Tavily Search API, optimized for AI applications.\n",
    "\n",
    "### Key Features:\n",
    "- üåê **Real-time web search** - Get the latest information from the internet\n",
    "- üéØ **AI-optimized** - Returns clean, relevant content for LLMs\n",
    "- üîí **Source attribution** - Includes URLs and metadata\n",
    "- ‚ö° **Fast & reliable** - Built specifically for AI use cases\n",
    "\n",
    "### Use Cases:\n",
    "- üì∞ Current events and news\n",
    "- üíπ Stock prices and market data\n",
    "- üå¶Ô∏è Weather information\n",
    "- üè¢ Company information\n",
    "- üîß Technical documentation and tutorials\n",
    "\n",
    "### Getting Started:\n",
    "1. Sign up at https://tavily.com/ (free tier available)\n",
    "2. Get your API key\n",
    "3. Add to `.env` file: `TAVILY_API_KEY=your_api_key`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic TavilySearchAPIRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m13 packages\u001b[0m \u001b[2min 390ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 21ms\u001b[0m\u001b[0m                                               \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m13                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtavily-python\u001b[0m\u001b[2m==0.7.13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 3 web results for 'latest developments in artificial intelligence 2024'\n",
      "\n",
      "================================================================================\n",
      "Source: https://www.launchconsulting.com/posts/the-future-of-business-ai-innovations-to-watch-in-2024\n",
      "\n",
      "Content (first 500 chars):\n",
      "AI Trends in 2024 ¬∑ 1. Generative AI: Beyond Chatbots ¬∑ 2. The Emergence of Small Language Models ¬∑ 3. Multi-Modal AI Experiences ¬∑ 4. AI Empowerment for...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a TavilySearchAPIRetriever instance\n",
    "# Make sure TAVILY_API_KEY is set in your .env file\n",
    "\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)  # Return top 3 results\n",
    "\n",
    "# Search for \"latest developments in artificial intelligence 2024\"\n",
    "query = \"latest developments in artificial intelligence 2024\"\n",
    "docs = tavily_retriever.invoke(query)\n",
    "\n",
    "print(f\"üîç Found {len(docs)} web results for '{query}'\\n\")\n",
    "\n",
    "# Display first result\n",
    "print(\"=\" * 80)\n",
    "print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "print(f\"\\nContent (first 500 chars):\\n{docs[0].page_content[:500]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced TavilySearchAPIRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Retrieved 5 web results\n",
      "\n",
      "1. Source: https://www.elastic.co/blog/langchain-tutorial\n",
      "   Content preview: LangChain even provides standard interfaces for a few of their main modules, including memory modules (a reusable building block that stores and manages data for use by large language models) and agen...\n",
      "\n",
      "2. Source: https://www.datacamp.com/tutorial/how-to-build-llm-applications-with-langchain\n",
      "   Content preview: This tutorial is perfect for you. Here, we explore LangChain - An open-source Python framework for building applications based on Large Language Models such as...\n",
      "\n",
      "3. Source: https://medium.com/data-science-in-your-pocket/langchain-tutorials-for-newbies-945319df04e2\n",
      "   Content preview: ## LangChain in your Pocket: Beginner's Guide to Building Generative AI Applications using LLMs ### LangChain in your Pocket: Beginner's Guide to Building Generative AI Applications using LLMs eBook :...\n",
      "\n",
      "4. Source: https://docs.langchain.com/oss/python/learn\n",
      "   Content preview: [LangChain](/oss/python/langchain/overview)[LangGraph](/oss/python/langgraph/overview)[Deep Agents](/oss/python/deepagents/overview)[Integrations](/oss/python/integrations/providers/overview)[Learn](/...\n",
      "\n",
      "5. Source: https://github.com/gkamradt/langchain-tutorials\n",
      "   Content preview: 2. LangChain CookBook Part 2: 9 Use Cases - Code, Video | Kor | Eugene Yurtsev | üêí Intermediate | ‚úÖ Code | This is a half-baked prototype that ‚Äúhelps‚Äù you extract structured data from text using large...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Control search depth and domain filtering\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "# Advanced configuration\n",
    "tavily_retriever_advanced = TavilySearchAPIRetriever(\n",
    "    k=5,  # Return top 5 results\n",
    "    # search_depth=\"advanced\",  # \"basic\" or \"advanced\" (more thorough)\n",
    "    # include_domains=[\"github.com\", \"stackoverflow.com\"],  # Filter to specific domains\n",
    "    # exclude_domains=[\"example.com\"]  # Exclude specific domains\n",
    ")\n",
    "\n",
    "# Search for \"LangChain tutorials\"\n",
    "query = \"LangChain tutorials\"\n",
    "docs = tavily_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üîç Retrieved {len(docs)} web results\\n\")\n",
    "\n",
    "# Display all results with sources\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content preview: {doc.page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Real-Time Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Real-Time Information (as of November 23, 2025):\n",
      "\n",
      "Query: 'latest AI news November 23, 2025'\n",
      "  ‚Üí Catch up on select AI news and developments from the past week or so: Google launches Gemini 3 and bakes it into search from Day One....\n",
      "  ‚Üí Source: https://www.marketingprofs.com/opinions/2025/54030/ai-update-november-21-2025-ai-news-and-views-from-the-past-week\n",
      "\n",
      "Query: 'current weather in San Francisco'\n",
      "  ‚Üí {'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1763878604, 'localtime': '2025-11-22 22:16'}, 'current': {'last_...\n",
      "  ‚Üí Source: https://www.weatherapi.com/\n",
      "\n",
      "Query: 'NVIDIA stock price today'\n",
      "  ‚Üí The NVIDIA Corporation Common Stock (NVDA) stock price today is $180.03, reflecting a -0.49% move since the market opened. The company's market capitalization...\n",
      "  ‚Üí Source: https://www.kraken.com/stocks/nvda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Get current information (news, weather, stock prices, etc.)\n",
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "# Real-time queries\n",
    "queries = [\n",
    "    f\"latest AI news {current_date}\",\n",
    "    \"current weather in San Francisco\",\n",
    "    \"NVIDIA stock price today\"\n",
    "]\n",
    "\n",
    "tavily_realtime = TavilySearchAPIRetriever(k=2)\n",
    "\n",
    "print(f\"üïê Real-Time Information (as of {current_date}):\\n\")\n",
    "\n",
    "for query in queries:\n",
    "    docs = tavily_realtime.invoke(query)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí {docs[0].page_content[:250]}...\")\n",
    "        print(f\"  ‚Üí Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding TavilySearchAPIRetriever Metadata\n",
    "\n",
    "Each document returned by TavilySearchAPIRetriever contains:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'source': 'https://example.com/...',  # Source URL\n",
    "    'score': 0.95,                         # Relevance score (0-1)\n",
    "    'title': 'Page Title',                 # Web page title (if available)\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the extracted text content from the web page.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rag'></a>\n",
    "## 6. Integration with RAG Chains üîó\n",
    "\n",
    "Now let's combine external retrievers with LLMs to build powerful **Retrieval-Augmented Generation (RAG)** systems!\n",
    "\n",
    "### üî∞ BEGINNER: Simple QA Chain with External Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is quantum computing and how does it work?\n",
      "\n",
      "Answer: - A quantum computer is a computer that uses quantum states‚Äîspecifically superposition and entanglement‚Äîand the probabilistic outcomes of quantum measurements as resources for computing.\n",
      "- The basic unit is the qubit, which can be in a superposition of 0 and 1, not just a single classical value.\n",
      "- Computation proceeds by manipulating qubits with quantum operations to create interference among many possible states. This interference amplifies the probability of the desired measurement result.\n",
      "- When you measure the qubits, you get one of the basis states with certain probabilities, dictated by the quantum state.\n",
      "- The goal of quantum algorithms is to design procedures that steer the system so the correct outcomes become more likely (amplitude amplification), effectively exploring a huge number of possibilities in parallel.\n",
      "- Quantum computers are not yet practical for everyday tasks; current hardware is largely experimental and suitable only for specialized problems.\n",
      "- They are believed to offer exponential speedups for some problems, such as breaking certain public-key cryptographic schemes, though this remains theoretical until scalable devices exist.\n"
     ]
    }
   ],
   "source": [
    "# Build a simple RAG chain using WikipediaRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Initialize components\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2, doc_content_chars_max=2000)\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Answer the question based on the following context from Wikipedia:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": wiki_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is quantum computing and how does it work?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Multi-Source RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are transformers in machine learning?\n",
      "\n",
      "Answer (from multiple sources):\n",
      "Transformers are a class of neural network architectures designed to process sequential data (most famously text) by using attention mechanisms to model relationships between elements in a sequence, without relying on traditional recurrence.\n",
      "\n",
      "What makes transformers special (core ideas)\n",
      "- Attention over tokens: The key idea is the attention mechanism, which lets the model weigh other positions in the input when encoding a given element (token). This allows the model to focus on the most relevant parts of the sequence for each token.\n",
      "- Multi-head attention: Instead of a single attention view, transformers compute several attention patterns in parallel (heads). This lets the model capture different kinds of relationships and dependencies simultaneously.\n",
      "- Embeddings and tokens: Text (or other sequential data) is first tokenized and each token is mapped to a vector via an embedding table. This provides a continuous, trainable representation for each element.\n",
      "- Contextual representations: Through layers of self-attention and feed-forward networks, each token‚Äôs representation becomes contextualized by the entire sequence, not just its neighbor in a fixed order.\n",
      "- Positional information: Because the basic attention mechanism is order-agnostic, transformers incorporate positional encodings to inject information about token order and sequence structure.\n",
      "- Encoder‚Äìdecoder structure (original form): The seminal transformer architecture pairs an encoder (which builds contextual representations of the input) with a decoder (which generates outputs, e.g., in translation). Many later models adapt parts of this design for specific tasks.\n",
      "\n",
      "What a transformer does in practice ( How it processes data)\n",
      "- Input: A sequence is tokenized and each token is converted to a vector via embeddings.\n",
      "- Encoding: The encoder stacks multiple layers of multi-head self-attention and feed-forward networks, with residual connections and normalization. Each layer refines token representations by attending to all other tokens.\n",
      "- Decoding (when used for sequence-to-sequence tasks): The decoder attends to both its own previous outputs and the encoder‚Äôs final representations to generate the next token, producing outputs autoregressively.\n",
      "- Outputs: For language tasks, outputs are probabilities over the vocabulary for the next token; for other tasks, the transformer can be adapted to produce structured representations, classifications, or sequences.\n",
      "\n",
      "Why transformers are powerful and widely used\n",
      "- Parallelism and scalability: Because attention computations can be performed across tokens in parallel, transformers train faster on modern hardware than recurrent models for long sequences.\n",
      "- Long-range dependencies: Self-attention can directly connect distant positions in a sequence, making it easier to model long-range relationships.\n",
      "- Broad applicability: While originated for natural language processing, the transformer paradigm has been extended to vision (Vision Transformers), speech, time series, and multi-modal data, and it underpins large language models and many modern AI systems.\n",
      "- Foundational status: The transformer architecture was popularized by the 2017 paper ‚ÄúAttention Is All You Need,‚Äù and it is considered a foundational development contributing to the AI surge and to the emergence of large language models.\n",
      "\n",
      "Relation to the sources you provided\n",
      "- From general knowledge: The transformer is a neural network architecture built around multi-head attention, operating on tokenized inputs (embedded as vectors) and contextualized through layers of attention. The approach uses positional information to retain order and has become foundational in modern AI, with the landmark 2017 paper Attention Is All You Need describing this design and its impact.\n",
      "- From the Wikipedia excerpt: It emphasizes tokens, embeddings, parallel multi-head attention, and contextualization of each token within a context window, via the attention mechanism. It also notes that Attention Is All You Need introduced the transformer and that this architecture became a main contributor to the AI boom.\n",
      "- From the academic arXiv fragments: While they discuss broader ML practices (e.g., rigorous validation workflows for ML in biology) and other architectures (e.g., PointNet-based embedding pipelines), these points illustrate that ML models, including complex ones like transformers, are deployed in rigorous scientific contexts and often require careful validation, data handling, and domain adaptation. The references also show that modern ML pipelines integrate embeddings and attention-like ideas as part of broader data-processing architectures.\n",
      "\n",
      "Bottom line\n",
      "- A transformer is a neural network architecture that leverages multi-head self-attention to build rich, contextual representations of sequential data, enabling highly scalable and effective models for a wide range of tasks, most famously in natural language processing and large language models. It replaces many recurrence-based approaches with parallelizable attention mechanisms, uses token embeddings plus positional information, and can be configured in encoder-only, decoder-only, or encoder‚Äìdecoder forms depending on the task.\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Combine multiple retrievers for comprehensive answers\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Initialize multiple retrievers\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=2)\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2, doc_content_chars_max=1500)\n",
    "\n",
    "# Function to combine results from multiple retrievers\n",
    "def multi_retriever(query):\n",
    "    \"\"\"Retrieve from multiple sources and combine results.\"\"\"\n",
    "    arxiv_docs = arxiv_retriever.invoke(query)\n",
    "    wiki_docs = wiki_retriever.invoke(query)\n",
    "    \n",
    "    # Combine and format\n",
    "    all_docs = []\n",
    "    \n",
    "    if arxiv_docs:\n",
    "        all_docs.append(\"=== Academic Papers (ArXiv) ===\")\n",
    "        all_docs.extend([doc.page_content[:500] for doc in arxiv_docs])\n",
    "    \n",
    "    if wiki_docs:\n",
    "        all_docs.append(\"\\n=== General Knowledge (Wikipedia) ===\")\n",
    "        all_docs.extend([doc.page_content[:500] for doc in wiki_docs])\n",
    "    \n",
    "    return \"\\n\\n\".join(all_docs)\n",
    "\n",
    "# Create multi-source RAG chain\n",
    "multi_source_template = \"\"\"Answer the question using information from multiple sources below:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive answer that synthesizes information from both academic and general sources:\"\"\"\n",
    "\n",
    "multi_prompt = ChatPromptTemplate.from_template(multi_source_template)\n",
    "\n",
    "multi_rag_chain = (\n",
    "    {\"context\": multi_retriever, \"question\": RunnablePassthrough()}\n",
    "    | multi_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What are transformers in machine learning?\"\n",
    "answer = multi_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer (from multiple sources):\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Real-Time RAG with TavilySearchAPIRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the latest developments in AI regulation?\n",
      "\n",
      "Answer (from real-time web search):\n",
      "As of the latest developments in AI regulation, countries around the world are increasingly recognizing the need for comprehensive laws and guidelines to govern the use of artificial intelligence. The European Union's AI Act, which was proposed in April 2021 and is set to come into effect on August 1, 2024, is one of the most significant pieces of AI regulation to date. This regulation aims to establish harmonized rules for AI across all 27 EU member states, addressing issues such as transparency, accountability, and data protection.\n",
      "\n",
      "One of the key provisions of the EU AI Act is the creation of a European Artificial Intelligence Board, which will be responsible for overseeing the implementation of the regulation and ensuring compliance with its provisions. The board will also be tasked with issuing guidance on the application of the regulation and promoting cooperation between EU member states in the field of AI.\n",
      "\n",
      "In addition to the EU's efforts, the United States has also taken steps to regulate AI. In June 2021, President Joe Biden signed an Executive Order on Removing Barriers to AI Leadership, which aims to promote the responsible use of AI while ensuring that the United States remains a global leader in AI innovation. The executive order directs federal agencies to develop plans for implementing AI principles and guidelines, as well as to prioritize AI research and development in their budgets.\n",
      "\n",
      "China, another major player in the field of AI, has also enacted targeted AI laws and is working on broader AI regulation. In 2017, China released a national AI development plan, which set out ambitious goals for the country to become a world leader in AI by 2030. Since then, China has implemented a number of regulations governing the use of AI in various sectors, such as healthcare, finance, and transportation.\n",
      "\n",
      "Despite these efforts, there are still many challenges to overcome in the regulation of AI. One of the biggest challenges is the rapid pace of technological advancement, which often outpaces the ability of regulators to keep up. Additionally, there are concerns about the potential for bias and discrimination in AI systems, as well as the need to protect privacy and data security.\n",
      "\n",
      "To address these challenges, regulators are increasingly turning to a combination of legal frameworks, ethical guidelines, and technical standards to govern the use of AI. For example, the EU AI Act includes provisions on transparency and accountability, requiring AI systems to be designed in a way that allows for human oversight and intervention. Similarly, the US Executive Order on AI emphasizes the importance of promoting AI that is trustworthy, transparent, and accountable.\n",
      "\n",
      "Overall, the latest developments in AI regulation reflect a growing recognition of the need to balance the benefits of AI with the potential risks and challenges it poses. By implementing comprehensive laws and guidelines, countries around the world are taking important steps towards ensuring that AI is used in a responsible and ethical manner.\n"
     ]
    }
   ],
   "source": [
    "# Build a RAG chain that uses real-time web search\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create prompt for real-time information\n",
    "realtime_template = \"\"\"Based on the latest information from the web:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide an up-to-date answer having atleast 500 words with source attribution:\"\"\"\n",
    "\n",
    "realtime_prompt = ChatPromptTemplate.from_template(realtime_template)\n",
    "\n",
    "# Build real-time RAG chain\n",
    "realtime_rag_chain = (\n",
    "    {\"context\": tavily_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | realtime_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a current events question\n",
    "question = \"What are the latest developments in AI regulation?\"\n",
    "answer = realtime_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer (from real-time web search):\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "## 7. Comparison & Use Cases üìä\n",
    "\n",
    "### Retriever Comparison Table\n",
    "\n",
    "| Feature | ArxivRetriever | WikipediaRetriever | TavilySearchAPIRetriever |\n",
    "|---------|----------------|-------------------|-------------------------|\n",
    "| **Data Source** | Academic papers (arxiv.org) | Wikipedia articles | Real-time web search |\n",
    "| **API Key Required** | ‚ùå No | ‚ùå No | ‚úÖ Yes (free tier) |\n",
    "| **Data Freshness** | Recent research | Regularly updated | Real-time |\n",
    "| **Best For** | Academic research, ML papers | General knowledge, definitions | Current events, news |\n",
    "| **Content Type** | Research papers, abstracts | Encyclopedia articles | Web pages, news |\n",
    "| **Default Results** | 3 papers | 3 articles | 5 results |\n",
    "| **Multilingual** | ‚ùå No | ‚úÖ Yes (300+ languages) | ‚úÖ Yes |\n",
    "| **Metadata** | Title, Authors, Published date | Title, Summary, URL | Source URL, Score |\n",
    "| **Rate Limits** | Moderate | Moderate | API-dependent |\n",
    "| **Cost** | üÜì Free | üÜì Free | üÜì Free tier + paid |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Each Retriever\n",
    "\n",
    "#### ‚úÖ Use **ArxivRetriever** when:\n",
    "- You need peer-reviewed academic research\n",
    "- You're building an AI/ML research assistant\n",
    "- You want the latest scientific papers\n",
    "- You need citations and author information\n",
    "\n",
    "#### ‚úÖ Use **WikipediaRetriever** when:\n",
    "- You need general knowledge and definitions\n",
    "- You want historical or biographical information\n",
    "- You're building an educational chatbot\n",
    "- You need multilingual support\n",
    "- You want reliable, community-edited content\n",
    "\n",
    "#### ‚úÖ Use **TavilySearchAPIRetriever** when:\n",
    "- You need real-time, up-to-date information\n",
    "- You're answering current events questions\n",
    "- You want to search the broader internet\n",
    "- You need to filter by specific domains\n",
    "- Your use case requires the latest data\n",
    "\n",
    "---\n",
    "\n",
    "### Combining Retrievers (Hybrid Approach)\n",
    "\n",
    "For the most comprehensive RAG system:\n",
    "\n",
    "```python\n",
    "# Pseudo-code for hybrid retrieval\n",
    "if query_type == \"academic\":\n",
    "    use ArxivRetriever\n",
    "elif query_type == \"general_knowledge\":\n",
    "    use WikipediaRetriever\n",
    "elif query_type == \"current_events\":\n",
    "    use TavilySearchAPIRetriever\n",
    "else:\n",
    "    # Use multiple retrievers and combine results\n",
    "    combine(ArxivRetriever, WikipediaRetriever, TavilySearchAPIRetriever)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best-practices'></a>\n",
    "## 8. Best Practices üí°\n",
    "\n",
    "### General Best Practices\n",
    "\n",
    "#### 1. **Handle Errors Gracefully**\n",
    "\n",
    "```python\n",
    "try:\n",
    "    docs = retriever.invoke(query)\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving documents: {e}\")\n",
    "    docs = []  # Fallback to empty list\n",
    "```\n",
    "\n",
    "#### 2. **Set Appropriate Limits**\n",
    "\n",
    "```python\n",
    "# Don't retrieve too many documents (costs, latency)\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=3)  # ‚úÖ Good\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=100)  # ‚ùå Too many\n",
    "```\n",
    "\n",
    "#### 3. **Cache Results for Repeated Queries**\n",
    "\n",
    "```python\n",
    "# Use a simple cache to avoid redundant API calls\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def cached_search(query: str):\n",
    "    return retriever.invoke(query)\n",
    "```\n",
    "\n",
    "#### 4. **Verify Source Attribution**\n",
    "\n",
    "```python\n",
    "# Always include sources in your responses\n",
    "for doc in docs:\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "```\n",
    "\n",
    "#### 5. **Combine with Vector Store Retrievers**\n",
    "\n",
    "```python\n",
    "# Use external retrievers for general knowledge\n",
    "# Use vector stores for your proprietary data\n",
    "def hybrid_retrieve(query):\n",
    "    external_docs = wiki_retriever.invoke(query)\n",
    "    internal_docs = vector_store.similarity_search(query)\n",
    "    return external_docs + internal_docs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Retriever-Specific Best Practices\n",
    "\n",
    "#### ArxivRetriever:\n",
    "- ‚úÖ Use specific search terms (e.g., \"BERT transformers\" vs \"AI\")\n",
    "- ‚úÖ Limit results to 3-5 papers for LLM context\n",
    "- ‚úÖ Extract metadata for citations\n",
    "- ‚ùå Don't use for non-academic queries\n",
    "\n",
    "#### WikipediaRetriever:\n",
    "- ‚úÖ Use for general knowledge, not specialized topics\n",
    "- ‚úÖ Set `doc_content_chars_max` to avoid huge documents\n",
    "- ‚úÖ Verify information for critical use cases\n",
    "- ‚ùå Don't rely on Wikipedia for real-time information\n",
    "\n",
    "#### TavilySearchAPIRetriever:\n",
    "- ‚úÖ Monitor API usage (rate limits, costs)\n",
    "- ‚úÖ Use for time-sensitive queries\n",
    "- ‚úÖ Filter by domain for specific sources\n",
    "- ‚ùå Don't use for queries that don't need real-time data\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Use `.batch()` for multiple queries**\n",
    "   ```python\n",
    "   # ‚úÖ Efficient\n",
    "   results = retriever.batch([q1, q2, q3])\n",
    "   \n",
    "   # ‚ùå Inefficient\n",
    "   results = [retriever.invoke(q) for q in [q1, q2, q3]]\n",
    "   ```\n",
    "\n",
    "2. **Limit document length for LLM context**\n",
    "   ```python\n",
    "   # Truncate long documents to fit LLM context window\n",
    "   docs = [Document(page_content=doc.page_content[:2000], metadata=doc.metadata) \n",
    "           for doc in raw_docs]\n",
    "   ```\n",
    "\n",
    "3. **Use async methods for concurrent retrieval** (if supported)\n",
    "   ```python\n",
    "   # For async-compatible retrievers\n",
    "   import asyncio\n",
    "   docs = await retriever.ainvoke(query)\n",
    "   ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 9. Summary & Exercises üìù\n",
    "\n",
    "### üéØ What You Learned\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "‚úÖ **External Index Retrievers** - Search over external data sources (internet, databases)\n",
    "\n",
    "‚úÖ **ArxivRetriever** - Retrieve academic papers from arxiv.org\n",
    "   - Use cases: Research, ML papers, citations\n",
    "   - Methods: `.invoke()`, `.batch()`\n",
    "   - Metadata: Title, Authors, Published date\n",
    "\n",
    "‚úÖ **WikipediaRetriever** - Access Wikipedia articles\n",
    "   - Use cases: General knowledge, definitions, history\n",
    "   - Features: Multilingual support, customizable length\n",
    "   - Metadata: Title, Summary, Source URL\n",
    "\n",
    "‚úÖ **TavilySearchAPIRetriever** - Real-time web search\n",
    "   - Use cases: Current events, news, real-time data\n",
    "   - Features: Domain filtering, search depth control\n",
    "   - Metadata: Source URL, Relevance score\n",
    "\n",
    "‚úÖ **RAG Integration** - Combined external retrievers with LLMs\n",
    "   - Built simple QA chains\n",
    "   - Created multi-source RAG systems\n",
    "   - Implemented real-time information retrieval\n",
    "\n",
    "‚úÖ **Best Practices** - Error handling, caching, source attribution\n",
    "\n",
    "---\n",
    "\n",
    "### üí™ Practice Exercises\n",
    "\n",
    "#### Exercise 1: Academic Research Assistant (üî∞ Beginner)\n",
    "Create a RAG chain that:\n",
    "- Uses `ArxivRetriever` to find papers on \"deep learning\"\n",
    "- Extracts the top 3 paper titles and authors\n",
    "- Summarizes each paper's abstract using an LLM\n",
    "\n",
    "#### Exercise 2: Wikipedia Fact Checker (üî∞ Beginner)\n",
    "Build a system that:\n",
    "- Takes a statement as input (e.g., \"Python was created in 1991\")\n",
    "- Uses `WikipediaRetriever` to search for relevant articles\n",
    "- Uses an LLM to verify if the statement is accurate\n",
    "\n",
    "#### Exercise 3: Multi-Source News Aggregator (üéì Intermediate)\n",
    "Create a RAG chain that:\n",
    "- Uses `TavilySearchAPIRetriever` to get latest AI news\n",
    "- Uses `WikipediaRetriever` to get background on AI topics\n",
    "- Combines both sources to provide a comprehensive news summary\n",
    "\n",
    "#### Exercise 4: Hybrid Retrieval System (üéì Intermediate)\n",
    "Build a system that:\n",
    "- Classifies queries into \"academic\", \"general\", or \"current_events\"\n",
    "- Routes to the appropriate retriever based on query type\n",
    "- Returns results from the most relevant source\n",
    "\n",
    "#### Exercise 5: Multilingual Knowledge Base (üöÄ Advanced)\n",
    "Create a system that:\n",
    "- Detects the language of the user's query\n",
    "- Uses `WikipediaRetriever` with the appropriate language setting\n",
    "- Returns answers in the user's language\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Next Steps\n",
    "\n",
    "- **Notebook 09**: Advanced Retrieval Techniques (Hybrid Search, Re-ranking)\n",
    "- **Notebook 10**: Production RAG Systems (Caching, Monitoring, Scaling)\n",
    "- **LangChain Documentation**: https://python.langchain.com/docs/integrations/retrievers/\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "- **ArXiv**: https://arxiv.org/\n",
    "- **Wikipedia API**: https://www.mediawiki.org/wiki/API:Main_page\n",
    "- **Tavily API**: https://tavily.com/\n",
    "- **LangChain Retrievers**: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You've mastered external index retrievers in LangChain!\n",
    "\n",
    "You can now build RAG systems that access:\n",
    "- üìÑ Academic research (ArXiv)\n",
    "- üìñ General knowledge (Wikipedia)\n",
    "- üåê Real-time web data (Tavily)\n",
    "\n",
    "Keep experimenting and building amazing AI applications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
