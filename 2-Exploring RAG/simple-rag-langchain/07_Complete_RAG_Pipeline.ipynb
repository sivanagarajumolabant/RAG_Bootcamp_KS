{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Notebook 07: Complete RAG Pipeline\n",
    "\n",
    "**LangChain 1.0.5+ | Mixed Level Class**\n",
    "\n",
    "## üéØ Objectives\n",
    "1. Build a complete RAG application\n",
    "2. Use LCEL to chain components\n",
    "3. Create production-ready code\n",
    "4. Handle errors properly\n",
    "5. Implement best practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "load_dotenv()\n",
    "print(\"‚úÖ Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Complete RAG Architecture\n",
    "\n",
    "### üî∞ BEGINNER\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "Retriever (finds relevant docs)\n",
    "    ‚Üì\n",
    "Format Context\n",
    "    ‚Üì\n",
    "Prompt Template\n",
    "    ‚Üì\n",
    "LLM\n",
    "    ‚Üì\n",
    "Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Step-by-Step RAG Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "  ‚úÖ Loaded notes.txt\n",
      "  ‚úÖ Loaded products.csv\n",
      "\n",
      "üìÑ Total documents: 16\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Documents\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader\n",
    "\n",
    "def load_all_documents(data_dir=\"sample_data\"):\n",
    "    \"\"\"Load documents from multiple sources\"\"\"\n",
    "    all_docs = []\n",
    "    \n",
    "    # Load text files\n",
    "    for txt_file in Path(data_dir).glob(\"*.txt\"):\n",
    "        loader = TextLoader(str(txt_file))\n",
    "        all_docs.extend(loader.load())\n",
    "        print(f\"  ‚úÖ Loaded {txt_file.name}\")\n",
    "    \n",
    "    # Load CSVs\n",
    "    for csv_file in Path(data_dir).glob(\"*.csv\"):\n",
    "        loader = CSVLoader(str(csv_file))\n",
    "        all_docs.extend(loader.load())\n",
    "        print(f\"  ‚úÖ Loaded {csv_file.name}\")\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "print(\"Loading documents...\")\n",
    "documents = load_all_documents()\n",
    "print(f\"\\nüìÑ Total documents: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting documents...\n",
      "‚úÇÔ∏è Created 27 chunks\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split Documents\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"Splitting documents...\")\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"‚úÇÔ∏è Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n",
      "Creating new vector store (this may take a minute)...\n",
      "‚úÖ Vector store ready\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Create Vector Store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "print(\"Creating embeddings...\")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Check if index exists\n",
    "index_path = \"./rag_vectorstore\"\n",
    "if Path(index_path).exists():\n",
    "    print(\"Loading existing vector store...\")\n",
    "    vectorstore = FAISS.load_local(\n",
    "        index_path,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Creating new vector store (this may take a minute)...\")\n",
    "    vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "    vectorstore.save_local(index_path)\n",
    "\n",
    "print(\"‚úÖ Vector store ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retriever created\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create Retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prompt template created\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create Prompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant. Answer the question based on the context below.\n",
    "If you cannot answer based on the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "print(\"‚úÖ Prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0  # Deterministic for factual answers\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the RAG Chain with LCEL\n",
    "\n",
    "### üéì INTERMEDIATE: LCEL Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain created!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,  # Retrieve and format docs\n",
    "        \"question\": RunnablePassthrough()    # Pass through the question\n",
    "    }\n",
    "    | prompt          # Format the prompt\n",
    "    | llm             # Generate answer\n",
    "    | StrOutputParser()  # Extract text from response\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Question: What is RAG?\n",
      "======================================================================\n",
      "\n",
      "Answer:\n",
      "RAG stands for Retrieval-Augmented Generation.\n",
      "\n",
      "Source Documents:\n",
      "  1. notes.txt\n",
      "  2. notes.txt\n",
      "  3. notes.txt\n",
      "  4. notes.txt\n",
      "\n",
      "======================================================================\n",
      "Question: What are the recommended chunk sizes?\n",
      "======================================================================\n",
      "\n",
      "Answer:\n",
      "For general text, the recommended chunk size is 1000 characters with an overlap of 200.\n",
      "\n",
      "Source Documents:\n",
      "  1. notes.txt\n",
      "  2. notes.txt\n",
      "  3. notes.txt\n",
      "  4. notes.txt\n",
      "\n",
      "======================================================================\n",
      "Question: What embedding models are available?\n",
      "======================================================================\n",
      "\n",
      "Answer:\n",
      "The available embedding models are:\n",
      "1. OpenAI text-embedding-3-small\n",
      "2. OpenAI text-embedding-3-large\n",
      "3. HuggingFace all-MiniLM-L6-v2\n",
      "4. HuggingFace all-mpnet-base-v2\n",
      "5. Google Gemini embedding-001\n",
      "\n",
      "Source Documents:\n",
      "  1. notes.txt\n",
      "  2. notes.txt\n",
      "  3. notes.txt\n",
      "  4. notes.txt\n"
     ]
    }
   ],
   "source": [
    "# Ask questions!\n",
    "questions = [\n",
    "    \"What is RAG?\",\n",
    "    \"What are the recommended chunk sizes?\",\n",
    "    \"What embedding models are available?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"\\nAnswer:\\n{answer}\")\n",
    "    \n",
    "    # Show source documents\n",
    "    print(\"\\nSource Documents:\")\n",
    "    docs = retriever.invoke(question)\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        print(f\"  {i}. {Path(source).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Production-Ready Version\n",
    "\n",
    "### üéì ADVANCED: With Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the transformer architecture?\n",
      "\n",
      "Answer: I don't have enough information to answer that.\n",
      "\n",
      "Sources (4 documents):\n",
      "  - notes.txt\n",
      "  - notes.txt\n",
      "  - notes.txt\n",
      "  - notes.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't have enough information to answer that.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rag_query(question: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Production-ready RAG query function\n",
    "    \n",
    "    Args:\n",
    "        question: User question\n",
    "        verbose: Print source documents\n",
    "    \n",
    "    Returns:\n",
    "        Answer string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get answer\n",
    "        answer = rag_chain.invoke(question)\n",
    "        \n",
    "        if verbose:\n",
    "            # Retrieve source docs\n",
    "            docs = retriever.invoke(question)\n",
    "            \n",
    "            print(f\"\\nQuestion: {question}\")\n",
    "            print(f\"\\nAnswer: {answer}\")\n",
    "            print(f\"\\nSources ({len(docs)} documents):\")\n",
    "            for doc in docs:\n",
    "                print(f\"  - {Path(doc.metadata['source']).name}\")\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test it\n",
    "rag_query(\"What is the transformer architecture?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "\n",
    "### üéì ADVANCED: Stream tokens as they're generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain machine learning in simple terms\n",
      "\n",
      "Answer (streaming):\n",
      "I don't have enough information to answer that.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stream response\n",
    "question = \"Explain machine learning in simple terms\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Answer (streaming):\")\n",
    "\n",
    "for chunk in rag_chain.stream(question):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices Summary\n",
    "\n",
    "### ‚úÖ Production Checklist\n",
    "\n",
    "- ‚úÖ **Persist vector stores** (save_local)\n",
    "- ‚úÖ **Error handling** (try/except)\n",
    "- ‚úÖ **Logging** (track queries and performance)\n",
    "- ‚úÖ **Prompt engineering** (clear instructions)\n",
    "- ‚úÖ **Source attribution** (show where answers come from)\n",
    "- ‚úÖ **Testing** (evaluate with test questions)\n",
    "- ‚úÖ **Monitoring costs** (track API usage)\n",
    "- ‚úÖ **Rate limiting** (prevent abuse)\n",
    "- ‚úÖ **Caching** (cache common queries)\n",
    "- ‚úÖ **Metadata filtering** (improve precision)\n",
    "\n",
    "### üìä Optimization Tips\n",
    "\n",
    "1. **Chunk size:** Test 500, 1000, 1500 with your data\n",
    "2. **Retrieval k:** Start with 4, adjust based on quality\n",
    "3. **Embeddings:** text-embedding-3-small for cost/performance\n",
    "4. **LLM:** GPT-3.5-Turbo for speed, GPT-4 for quality\n",
    "5. **Temperature:** 0 for factual, 0.7 for creative\n",
    "\n",
    "## Summary\n",
    "\n",
    "üéâ **Congratulations!** You've built a complete RAG system!\n",
    "\n",
    "You now know:\n",
    "- ‚úÖ How to load and process documents\n",
    "- ‚úÖ Text splitting strategies\n",
    "- ‚úÖ Creating embeddings\n",
    "- ‚úÖ Using vector stores\n",
    "- ‚úÖ Retrieval strategies\n",
    "- ‚úÖ Building LCEL chains\n",
    "- ‚úÖ Production best practices\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "- Build a RAG app for your own documents\n",
    "- Experiment with different embedding models\n",
    "- Try advanced retrieval (hybrid search, re-ranking)\n",
    "- Add conversation memory\n",
    "- Deploy to production\n",
    "\n",
    "**Happy building! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
