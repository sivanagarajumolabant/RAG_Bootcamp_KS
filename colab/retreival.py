# -*- coding: utf-8 -*-
"""Retreival.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13GdLsnx3IcXptckMRBIR1dGXdbeDnPGA
"""

from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from torch.nn.functional import cosine_similarity
import torch

# Load pretrained DPR models
q_encoder = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")

p_encoder = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
p_tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")

# Encode a question
question = "Who wrote Hamlet?"
q_inputs = q_tokenizer(question, return_tensors='pt')
q_emb = q_encoder(**q_inputs).pooler_output

# Encode passages
passages = ["Shakespeare is the author of Hamlet.", "Paris is the capital of France."]
p_inputs = p_tokenizer(passages, return_tensors='pt', padding=True, truncation=True)
p_emb = p_encoder(**p_inputs).pooler_output

# Compute similarity
scores = cosine_similarity(q_emb, p_emb)
print(scores)  # higher score = more relevant

print(scores)

